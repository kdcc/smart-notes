#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæœ¬çš„æ™ºèƒ½æ–‡æœ¬åˆ†ææ¼”ç¤º
ä½¿ç”¨å†…å­˜å‘é‡å­˜å‚¨ï¼Œæ— éœ€å¤–éƒ¨QdrantæœåŠ¡
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

# åŸºç¡€åº“
import numpy as np

# LangChain imports
from langchain_community.document_loaders import TextLoader
from langchain.schema import Document

# Sentence Transformers for embeddings
from sentence_transformers import SentenceTransformer

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SimpleMetadataExtractor:
    """ç®€åŒ–çš„å…ƒæ•°æ®æå–å™¨"""
    
    def __init__(self):
        self.category_patterns = {
            'æŠ€æœ¯æ–‡æ¡£': [r'tech|æŠ€æœ¯|å¼€å‘|api|sdk|ä»£ç |guide'],
            'ç”¨æˆ·æ‰‹å†Œ': [r'manual|æ‰‹å†Œ|æŒ‡å—|user|ç”¨æˆ·'],
            'é¡¹ç›®èµ„æ–™': [r'project|é¡¹ç›®|éœ€æ±‚|requirement'],
            'ä¼šè®®è®°å½•': [r'meeting|ä¼šè®®|çºªè¦|è®°å½•|minutes'],
            'æŠ¥å‘Šæ–‡æ¡£': [r'report|æŠ¥å‘Š|åˆ†æ|analysis'],
            'å­¦ä¹ ç¬”è®°': [r'note|ç¬”è®°|å­¦ä¹ |study'],
            'å…¶ä»–': []
        }
    
    def extract_metadata(self, file_path: str) -> Dict[str, Any]:
        """ä»æ–‡ä»¶è·¯å¾„æå–å…ƒæ•°æ®"""
        path_obj = Path(file_path)
        filename = path_obj.stem.lower()
        
        metadata = {
            'filename': path_obj.name,
            'file_path': str(file_path),
            'category': 'å…¶ä»–',
            'tags': []
        }
        
        # åˆ¤æ–­åˆ†ç±»
        for category, patterns in self.category_patterns.items():
            if category == 'å…¶ä»–':
                continue
            for pattern in patterns:
                if re.search(pattern, filename, re.IGNORECASE):
                    metadata['category'] = category
                    break
            if metadata['category'] != 'å…¶ä»–':
                break
        
        return metadata


class SimpleTextChunker:
    """ç®€åŒ–çš„æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 500, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[Document]:
        """å°†æ–‡æœ¬åˆ†å—"""
        chunks = []
        start = 0
        chunk_id = 0
        
        while start < len(text):
            end = start + self.chunk_size
            chunk_text = text[start:end]
            
            # é¿å…åœ¨å¥å­ä¸­é—´åˆ†å‰²
            if end < len(text):
                last_period = chunk_text.rfind('ã€‚')
                last_newline = chunk_text.rfind('\n')
                if last_period > self.chunk_size // 2:
                    end = start + last_period + 1
                elif last_newline > self.chunk_size // 2:
                    end = start + last_newline + 1
            
            chunk_text = text[start:end].strip()
            if chunk_text:
                chunk_metadata = metadata.copy()
                chunk_metadata.update({
                    'chunk_id': chunk_id,
                    'chunk_size': len(chunk_text)
                })
                
                doc = Document(
                    page_content=chunk_text,
                    metadata=chunk_metadata
                )
                chunks.append(doc)
                chunk_id += 1
            
            start = end - self.overlap
        
        return chunks


class SimpleVectorStore:
    """ç®€åŒ–çš„å†…å­˜å‘é‡å­˜å‚¨"""
    
    def __init__(self):
        self.vectors = []
        self.documents = []
        self.metadata = []
    
    def add_documents(self, documents: List[Document], embeddings: np.ndarray):
        """æ·»åŠ æ–‡æ¡£å’Œå¯¹åº”çš„åµŒå…¥å‘é‡"""
        for i, doc in enumerate(documents):
            self.vectors.append(embeddings[i])
            self.documents.append(doc.page_content)
            self.metadata.append(doc.metadata)
    
    def similarity_search(
        self,
        query_embedding: np.ndarray,
        k: int = 5,
        score_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        if not self.vectors:
            return []
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        vectors_array = np.array(self.vectors)
        
        # å½’ä¸€åŒ–
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        vectors_norm = vectors_array / np.linalg.norm(vectors_array, axis=1, keepdims=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(vectors_norm, query_norm)
        
        # è·å–top-kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:k]
        
        results = []
        for idx in top_indices:
            score = similarities[idx]
            if score >= score_threshold:
                results.append({
                    'content': self.documents[idx],
                    'metadata': self.metadata[idx],
                    'score': float(score),
                    'id': int(idx)
                })
        
        return results
    
    def filter_search(
        self,
        query_embedding: np.ndarray,
        category_filter: Optional[str] = None,
        k: int = 5
    ) -> List[Dict[str, Any]]:
        """å¸¦è¿‡æ»¤çš„æœç´¢"""
        if not self.vectors:
            return []
        
        # å…ˆè¿‡æ»¤
        filtered_indices = []
        for i, meta in enumerate(self.metadata):
            if category_filter and meta.get('category') != category_filter:
                continue
            filtered_indices.append(i)
        
        if not filtered_indices:
            return []
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        filtered_vectors = np.array([self.vectors[i] for i in filtered_indices])
        query_norm = query_embedding / np.linalg.norm(query_embedding)
        vectors_norm = filtered_vectors / np.linalg.norm(filtered_vectors, axis=1, keepdims=True)
        similarities = np.dot(vectors_norm, query_norm)
        
        # è·å–top-k
        top_local_indices = np.argsort(similarities)[::-1][:k]
        
        results = []
        for local_idx in top_local_indices:
            original_idx = filtered_indices[local_idx]
            results.append({
                'content': self.documents[original_idx],
                'metadata': self.metadata[original_idx],
                'score': float(similarities[local_idx]),
                'id': original_idx
            })
        
        return results


class SimpleTextAnalyzer:
    """ç®€åŒ–çš„æ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self, model_name: str = "BAAI/bge-base-zh-v1.5"):
        self.model_name = model_name
        self.metadata_extractor = SimpleMetadataExtractor()
        self.text_chunker = SimpleTextChunker()
        self.vector_store = SimpleVectorStore()
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name}")
        self.embedding_model = SentenceTransformer(model_name)
        logger.info("åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_and_process_files(self, file_paths: List[str]) -> bool:
        """åŠ è½½å¹¶å¤„ç†æ–‡ä»¶"""
        try:
            all_chunks = []
            
            for file_path in file_paths:
                logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
                
                # åŠ è½½æ–‡æ¡£
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
                
                for doc in docs:
                    # æå–å…ƒæ•°æ®
                    metadata = self.metadata_extractor.extract_metadata(file_path)
                    doc.metadata.update(metadata)
                    
                    # åˆ†å—
                    chunks = self.text_chunker.chunk_text(doc.page_content, doc.metadata)
                    all_chunks.extend(chunks)
                    
                    logger.info(f"æ–‡æ¡£ {metadata['filename']} åˆ†æˆ {len(chunks)} å—")
            
            if not all_chunks:
                logger.error("æ²¡æœ‰å¤„ç†åˆ°ä»»ä½•æ–‡æ¡£å—")
                return False
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            logger.info("æ­£åœ¨ç”ŸæˆåµŒå…¥å‘é‡...")
            texts = [chunk.page_content for chunk in all_chunks]
            embeddings = self.embedding_model.encode(texts, show_progress_bar=True)
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            self.vector_store.add_documents(all_chunks, embeddings)
            
            logger.info(f"æˆåŠŸå¤„ç† {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def search(
        self,
        query: str,
        k: int = 5,
        category_filter: Optional[str] = None,
        score_threshold: float = 0.3
    ) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£"""
        try:
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = self.embedding_model.encode([query])[0]
            
            # æ‰§è¡Œæœç´¢
            if category_filter:
                results = self.vector_store.filter_search(
                    query_embedding, category_filter, k
                )
            else:
                results = self.vector_store.similarity_search(
                    query_embedding, k, score_threshold
                )
            
            logger.info(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        categories = {}
        for meta in self.vector_store.metadata:
            cat = meta.get('category', 'æœªçŸ¥')
            categories[cat] = categories.get(cat, 0) + 1
        
        return {
            'total_chunks': len(self.vector_store.documents),
            'categories': categories
        }


def main():
    """æ¼”ç¤ºä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– ç®€åŒ–ç‰ˆæ™ºèƒ½æ–‡æœ¬åˆ†æç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # æŸ¥æ‰¾txtæ–‡ä»¶
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    notes_dir = Path("./notes")
    if not notes_dir.exists():
        notes_dir = Path(".")
    current_dir = notes_dir
    txt_files = list(current_dir.glob("*.txt"))
    
    if not txt_files:
        print("âŒ æœªæ‰¾åˆ°txtæ–‡ä»¶ï¼Œè¯·å…ˆåˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶:")
    for f in txt_files:
        print(f"   - {f.name}")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    print("\nğŸ”„ æ­£åœ¨åˆå§‹åŒ–æ–‡æœ¬åˆ†æå™¨...")
    try:
        analyzer = SimpleTextAnalyzer()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤ºï¼šé¦–æ¬¡è¿è¡Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œè¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
        return
    
    # å¤„ç†æ–‡ä»¶
    print("\nğŸ“– æ­£åœ¨å¤„ç†æ–‡æ¡£...")
    file_paths = [str(f) for f in txt_files]
    success = analyzer.load_and_process_files(file_paths)
    
    if not success:
        print("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥")
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = analyzer.get_stats()
    print(f"\nğŸ“Š å¤„ç†å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»æ–‡æ¡£å—æ•°: {stats['total_chunks']}")
    print(f"   åˆ†ç±»åˆ†å¸ƒ:")
    for category, count in stats['categories'].items():
        print(f"     - {category}: {count}å—")
    
    # æ¼”ç¤ºæœç´¢
    print("\nğŸ” æœç´¢æ¼”ç¤º:")
    
    test_queries = [
        "å¦‚ä½•ä½¿ç”¨API",
        "é¡¹ç›®å¼€å‘è®¡åˆ’", 
        "ç”¨æˆ·ç™»å½•åŠŸèƒ½",
        "ä¼šè®®è®¨è®ºå†…å®¹"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        results = analyzer.search(query, k=2)
        
        if results:
            for i, result in enumerate(results, 1):
                print(f"   {i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
                print(f"      åˆ†ç±»: {result['metadata']['category']}")
                print(f"      æ–‡ä»¶: {result['metadata']['filename']}")
                print(f"      é¢„è§ˆ: {result['content'][:80]}...")
        else:
            print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    # æŒ‰åˆ†ç±»æœç´¢æ¼”ç¤º
    print(f"\nğŸ·ï¸  æŒ‰åˆ†ç±»æœç´¢æ¼”ç¤º:")
    category_query = "å¼€å‘"
    category_filter = "æŠ€æœ¯æ–‡æ¡£"
    print(f"æœç´¢ '{category_query}' (é™å®šåˆ†ç±»: {category_filter})")
    
    results = analyzer.search(category_query, k=2, category_filter=category_filter)
    if results:
        for i, result in enumerate(results, 1):
            print(f"   {i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
            print(f"      æ–‡ä»¶: {result['metadata']['filename']}")
            print(f"      é¢„è§ˆ: {result['content'][:80]}...")
    else:
        print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("   - ç³»ç»Ÿä¼šè‡ªåŠ¨æ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ–‡æ¡£ç±»å‹")
    print("   - æ”¯æŒä¸­æ–‡è¯­ä¹‰æœç´¢")
    print("   - å¯ä»¥æŒ‰åˆ†ç±»ç­›é€‰æœç´¢ç»“æœ")
    print("   - è¿”å›ç»“æœæŒ‰ç›¸ä¼¼åº¦æ’åº")


if __name__ == "__main__":
    main()
