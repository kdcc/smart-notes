#!/usr/bin/env python3
"""
æœ¬åœ°ç‰ˆæ™ºèƒ½æ–‡æœ¬åˆ†ææ¼”ç¤º
ä½¿ç”¨TF-IDFå‘é‡åŒ–ï¼Œæ— éœ€ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
"""

import os
import re
import logging
import jieba
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime

# åŸºç¡€åº“
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# LangChain imports
from langchain_community.document_loaders import TextLoader
from langchain.schema import Document

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LocalMetadataExtractor:
    """æœ¬åœ°å…ƒæ•°æ®æå–å™¨"""
    
    def __init__(self):
        self.category_patterns = {
            'æŠ€æœ¯æ–‡æ¡£': [r'tech|æŠ€æœ¯|å¼€å‘|api|sdk|ä»£ç |guide|å¼€å‘æŒ‡å—'],
            'ç”¨æˆ·æ‰‹å†Œ': [r'manual|æ‰‹å†Œ|æŒ‡å—|user|ç”¨æˆ·|ä½¿ç”¨'],
            'é¡¹ç›®èµ„æ–™': [r'project|é¡¹ç›®|éœ€æ±‚|requirement|æ–‡æ¡£'],
            'ä¼šè®®è®°å½•': [r'meeting|ä¼šè®®|çºªè¦|è®°å½•|minutes'],
            'æŠ¥å‘Šæ–‡æ¡£': [r'report|æŠ¥å‘Š|åˆ†æ|analysis'],
            'å­¦ä¹ ç¬”è®°': [r'note|ç¬”è®°|å­¦ä¹ |study'],
            'å…¶ä»–': []
        }
    
    def extract_metadata(self, file_path: str) -> Dict[str, Any]:
        """ä»æ–‡ä»¶è·¯å¾„æå–å…ƒæ•°æ®"""
        path_obj = Path(file_path)
        filename = path_obj.stem.lower()
        
        metadata = {
            'filename': path_obj.name,
            'file_path': str(file_path),
            'category': 'å…¶ä»–',
            'tags': [],
            'created_time': datetime.now().isoformat()
        }
        
        # åˆ¤æ–­åˆ†ç±»
        for category, patterns in self.category_patterns.items():
            if category == 'å…¶ä»–':
                continue
            for pattern in patterns:
                if re.search(pattern, filename, re.IGNORECASE):
                    metadata['category'] = category
                    break
            if metadata['category'] != 'å…¶ä»–':
                break
        
        # æå–æ ‡ç­¾
        tags = set()
        if any(word in filename for word in ['tech', 'æŠ€æœ¯', 'api']):
            tags.add('æŠ€æœ¯')
        if any(word in filename for word in ['user', 'ç”¨æˆ·', 'manual']):
            tags.add('ç”¨æˆ·')
        if any(word in filename for word in ['project', 'é¡¹ç›®']):
            tags.add('é¡¹ç›®')
        
        metadata['tags'] = list(tags)
        return metadata


class LocalTextChunker:
    """æœ¬åœ°æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 300, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_text(self, text: str, metadata: Dict[str, Any]) -> List[Document]:
        """å°†æ–‡æœ¬åˆ†å—"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        chunks = []
        current_chunk = ""
        chunk_id = 0
        
        for paragraph in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œå°±æ·»åŠ 
            if len(current_chunk) + len(paragraph) <= self.chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunk_metadata = metadata.copy()
                    chunk_metadata.update({
                        'chunk_id': chunk_id,
                        'chunk_size': len(current_chunk)
                    })
                    
                    doc = Document(
                        page_content=current_chunk,
                        metadata=chunk_metadata
                    )
                    chunks.append(doc)
                    chunk_id += 1
                
                # å¼€å§‹æ–°å—
                current_chunk = paragraph
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunk_metadata = metadata.copy()
            chunk_metadata.update({
                'chunk_id': chunk_id,
                'chunk_size': len(current_chunk)
            })
            
            doc = Document(
                page_content=current_chunk,
                metadata=chunk_metadata
            )
            chunks.append(doc)
        
        return chunks


class LocalVectorStore:
    """æœ¬åœ°TF-IDFå‘é‡å­˜å‚¨"""
    
    def __init__(self):
        self.documents = []
        self.metadata = []
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,
            ngram_range=(1, 2),
            tokenizer=self._tokenize
        )
        self.vectors = None
    
    def _tokenize(self, text: str) -> List[str]:
        """ä¸­æ–‡åˆ†è¯"""
        # ç®€å•çš„ä¸­æ–‡åˆ†è¯
        tokens = []
        # åˆ†å‰²ä¸­æ–‡å­—ç¬¦
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', text)
        for chars in chinese_chars:
            tokens.extend(list(chars))  # æŒ‰å­—ç¬¦åˆ†å‰²
        
        # åˆ†å‰²è‹±æ–‡å•è¯
        english_words = re.findall(r'[a-zA-Z]+', text)
        tokens.extend([w.lower() for w in english_words])
        
        # åˆ†å‰²æ•°å­—
        numbers = re.findall(r'\d+', text)
        tokens.extend(numbers)
        
        return tokens
    
    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æ¡£"""
        for doc in documents:
            self.documents.append(doc.page_content)
            self.metadata.append(doc.metadata)
        
        # ç”ŸæˆTF-IDFå‘é‡
        if self.documents:
            self.vectors = self.vectorizer.fit_transform(self.documents)
            logger.info(f"ç”Ÿæˆäº† {self.vectors.shape[0]} ä¸ªæ–‡æ¡£çš„ {self.vectors.shape[1]} ç»´TF-IDFå‘é‡")
    
    def similarity_search(
        self,
        query: str,
        k: int = 5,
        score_threshold: float = 0.1
    ) -> List[Dict[str, Any]]:
        """ç›¸ä¼¼åº¦æœç´¢"""
        if self.vectors is None or len(self.documents) == 0:
            return []
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
        query_vector = self.vectorizer.transform([query])
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vector, self.vectors).flatten()
        
        # è·å–top-kç»“æœ
        top_indices = similarities.argsort()[::-1][:k]
        
        results = []
        for idx in top_indices:
            score = similarities[idx]
            if score >= score_threshold:
                results.append({
                    'content': self.documents[idx],
                    'metadata': self.metadata[idx],
                    'score': float(score),
                    'id': int(idx)
                })
        
        return results
    
    def filter_search(
        self,
        query: str,
        category_filter: Optional[str] = None,
        k: int = 5
    ) -> List[Dict[str, Any]]:
        """å¸¦è¿‡æ»¤çš„æœç´¢"""
        if self.vectors is None:
            return []
        
        # å…ˆè¿‡æ»¤
        filtered_indices = []
        for i, meta in enumerate(self.metadata):
            if category_filter and meta.get('category') != category_filter:
                continue
            filtered_indices.append(i)
        
        if not filtered_indices:
            return []
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œå‘é‡åŒ–
        query_vector = self.vectorizer.transform([query])
        
        # è®¡ç®—è¿‡æ»¤åæ–‡æ¡£çš„ç›¸ä¼¼åº¦
        filtered_vectors = self.vectors[filtered_indices]
        similarities = cosine_similarity(query_vector, filtered_vectors).flatten()
        
        # è·å–top-k
        top_local_indices = similarities.argsort()[::-1][:k]
        
        results = []
        for local_idx in top_local_indices:
            original_idx = filtered_indices[local_idx]
            results.append({
                'content': self.documents[original_idx],
                'metadata': self.metadata[original_idx],
                'score': float(similarities[local_idx]),
                'id': original_idx
            })
        
        return results


class LocalTextAnalyzer:
    """æœ¬åœ°æ–‡æœ¬åˆ†æå™¨"""
    
    def __init__(self):
        self.metadata_extractor = LocalMetadataExtractor()
        self.text_chunker = LocalTextChunker()
        self.vector_store = LocalVectorStore()
        
        logger.info("æœ¬åœ°æ–‡æœ¬åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_and_process_files(self, file_paths: List[str]) -> bool:
        """åŠ è½½å¹¶å¤„ç†æ–‡ä»¶"""
        try:
            all_chunks = []
            
            for file_path in file_paths:
                logger.info(f"å¤„ç†æ–‡ä»¶: {file_path}")
                
                # åŠ è½½æ–‡æ¡£
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
                
                for doc in docs:
                    # æå–å…ƒæ•°æ®
                    metadata = self.metadata_extractor.extract_metadata(file_path)
                    doc.metadata.update(metadata)
                    
                    # åˆ†å—
                    chunks = self.text_chunker.chunk_text(doc.page_content, doc.metadata)
                    all_chunks.extend(chunks)
                    
                    logger.info(f"æ–‡æ¡£ {metadata['filename']} åˆ†æˆ {len(chunks)} å—")
            
            if not all_chunks:
                logger.error("æ²¡æœ‰å¤„ç†åˆ°ä»»ä½•æ–‡æ¡£å—")
                return False
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            self.vector_store.add_documents(all_chunks)
            
            logger.info(f"æˆåŠŸå¤„ç† {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
            return True
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return False
    
    def search(
        self,
        query: str,
        k: int = 5,
        category_filter: Optional[str] = None,
        score_threshold: float = 0.05
    ) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£"""
        try:
            # æ‰§è¡Œæœç´¢
            if category_filter:
                results = self.vector_store.filter_search(
                    query, category_filter, k
                )
            else:
                results = self.vector_store.similarity_search(
                    query, k, score_threshold
                )
            
            logger.info(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        categories = {}
        for meta in self.vector_store.metadata:
            cat = meta.get('category', 'æœªçŸ¥')
            categories[cat] = categories.get(cat, 0) + 1
        
        return {
            'total_chunks': len(self.vector_store.documents),
            'categories': categories,
            'vector_dims': self.vector_store.vectors.shape[1] if self.vector_store.vectors is not None else 0
        }


def main():
    """æ¼”ç¤ºä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– æœ¬åœ°ç‰ˆæ™ºèƒ½æ–‡æœ¬åˆ†æç³»ç»Ÿæ¼”ç¤º")
    print("ğŸ’¡ ä½¿ç”¨TF-IDFå‘é‡åŒ–ï¼Œæ— éœ€ä¸‹è½½æ¨¡å‹")
    print("=" * 60)
    
    # æŸ¥æ‰¾txtæ–‡ä»¶
    current_dir = Path("./notes")  # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    if not current_dir.exists():
        current_dir = Path(".")  # å¦‚æœnotesç›®å½•ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰ç›®å½•
    txt_files = list(current_dir.glob("*.txt"))
    
    if not txt_files:
        print("âŒ æœªæ‰¾åˆ°txtæ–‡ä»¶ï¼Œè¯·å…ˆåˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶:")
    for f in txt_files:
        print(f"   - {f.name}")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    print("\nğŸ”„ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°æ–‡æœ¬åˆ†æå™¨...")
    analyzer = LocalTextAnalyzer()
    
    # å¤„ç†æ–‡ä»¶
    print("\nğŸ“– æ­£åœ¨å¤„ç†æ–‡æ¡£...")
    file_paths = [str(f) for f in txt_files]
    success = analyzer.load_and_process_files(file_paths)
    
    if not success:
        print("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥")
        return
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = analyzer.get_stats()
    print(f"\nğŸ“Š å¤„ç†å®Œæˆï¼ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»æ–‡æ¡£å—æ•°: {stats['total_chunks']}")
    print(f"   å‘é‡ç»´åº¦: {stats['vector_dims']}")
    print(f"   åˆ†ç±»åˆ†å¸ƒ:")
    for category, count in stats['categories'].items():
        print(f"     - {category}: {count}å—")
    
    # æ¼”ç¤ºæœç´¢
    print("\nğŸ” æœç´¢æ¼”ç¤º:")
    
    test_queries = [
        "APIæ¥å£å¼€å‘",
        "ç”¨æˆ·ç™»å½•ç³»ç»Ÿ",
        "é¡¹ç›®éœ€æ±‚åˆ†æ", 
        "ä¼šè®®è®¨è®ºå†…å®¹",
        "æŠ€æœ¯å®ç°æ–¹æ¡ˆ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æœç´¢: '{query}'")
        results = analyzer.search(query, k=2)
        
        if results:
            for i, result in enumerate(results, 1):
                print(f"   {i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
                print(f"      åˆ†ç±»: {result['metadata']['category']}")
                print(f"      æ–‡ä»¶: {result['metadata']['filename']}")
                print(f"      é¢„è§ˆ: {result['content'][:80]}...")
        else:
            print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    # æŒ‰åˆ†ç±»æœç´¢æ¼”ç¤º
    print(f"\nğŸ·ï¸  æŒ‰åˆ†ç±»æœç´¢æ¼”ç¤º:")
    category_query = "å¼€å‘æŒ‡å—"
    category_filter = "æŠ€æœ¯æ–‡æ¡£"
    print(f"æœç´¢ '{category_query}' (é™å®šåˆ†ç±»: {category_filter})")
    
    results = analyzer.search(category_query, k=2, category_filter=category_filter)
    if results:
        for i, result in enumerate(results, 1):
            print(f"   {i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
            print(f"      æ–‡ä»¶: {result['metadata']['filename']}")
            print(f"      é¢„è§ˆ: {result['content'][:80]}...")
    else:
        print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ åŠŸèƒ½è¯´æ˜:")
    print("   âœ“ è‡ªåŠ¨æ–‡æ¡£åˆ†ç±»ï¼ˆæ ¹æ®æ–‡ä»¶åï¼‰")
    print("   âœ“ æ™ºèƒ½æ–‡æœ¬åˆ†å—")
    print("   âœ“ TF-IDFè¯­ä¹‰æœç´¢")
    print("   âœ“ æŒ‰åˆ†ç±»ç­›é€‰")
    print("   âœ“ ç›¸ä¼¼åº¦æ’åº")
    print("   âœ“ ä¸­è‹±æ–‡æ··åˆæ”¯æŒ")
    
    print("\nğŸ”§ æŠ€æœ¯ç‰¹ç‚¹:")
    print("   - æœ¬åœ°è¿è¡Œï¼Œæ— éœ€ç½‘ç»œ")
    print("   - è½»é‡çº§TF-IDFå‘é‡åŒ–")
    print("   - æ”¯æŒä¸­æ–‡æ–‡æœ¬å¤„ç†")
    print("   - å®æ—¶æœç´¢å“åº”")


if __name__ == "__main__":
    main()
