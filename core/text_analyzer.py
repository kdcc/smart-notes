#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬åˆ†æå’Œæ£€ç´¢ç³»ç»Ÿ
æ”¯æŒå¤šä¸ªtxtæ–‡ä»¶çš„åŠ è½½ã€åˆ†å—ã€åµŒå…¥å’Œæ™ºèƒ½æ£€ç´¢

åŠŸèƒ½ç‰¹æ€§ï¼š
- LangChain TextLoader + è‡ªå®šä¹‰å…ƒæ•°æ®æå–ï¼ˆä»æ–‡ä»¶åæå–åˆ†ç±»æ ‡ç­¾ï¼‰
- SemanticChunkerï¼ˆåŸºäºå¥å­ç›¸ä¼¼åº¦è‡ªåŠ¨åˆå¹¶ï¼Œé¿å…å‰²è£‚é•¿æ®µè½ï¼‰
- bge-base-zh-v1.5 åµŒå…¥æ¨¡å‹ï¼ˆä¸­æ–‡ä¼˜å…ˆï¼Œ1.3Bå‚æ•°ï¼ŒGPUæ¨ç†é€Ÿåº¦å¿«ï¼‰
- Qdrant å‘é‡åº“ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼Œæ”¯æŒæŒ‰"æ ‡ç­¾"æ£€ç´¢ï¼‰
"""

import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import hashlib
import json
import uuid

# LangChain imports
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.schema import Document

# Sentence Transformers for embeddings
from sentence_transformers import SentenceTransformer

# Ollama for local embeddings
import ollama
import numpy as np

# Qdrant for vector storage
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, VectorParams, PointStruct, Filter, 
    FieldCondition, MatchValue, Range
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class OllamaEmbedding:
    """Ollama åµŒå…¥æ¨¡å‹åŒ…è£…å™¨"""
    
    def __init__(self, model_name: str = "quentinz/bge-large-zh-v1.5"):
        self.model_name = model_name
        self.client = ollama.Client()
        
        # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
        try:
            # è¿›è¡Œä¸€æ¬¡æµ‹è¯•åµŒå…¥
            test_response = self.client.embeddings(
                model=self.model_name,
                prompt="æµ‹è¯•"
            )
            self.embedding_dim = len(test_response['embedding'])
            logger.info(f"Ollamaæ¨¡å‹ {model_name} åˆå§‹åŒ–æˆåŠŸï¼Œå‘é‡ç»´åº¦: {self.embedding_dim}")
        except Exception as e:
            logger.error(f"Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def encode(self, texts):
        """ç¼–ç æ–‡æœ¬ä¸ºå‘é‡"""
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = []
        for text in texts:
            try:
                response = self.client.embeddings(
                    model=self.model_name,
                    prompt=text
                )
                embeddings.append(response['embedding'])
            except Exception as e:
                logger.error(f"æ–‡æœ¬åµŒå…¥å¤±è´¥: {e}")
                # è¿”å›é›¶å‘é‡ä½œä¸ºfallback
                embeddings.append([0.0] * self.embedding_dim)
        
        return np.array(embeddings) if len(embeddings) > 1 else np.array(embeddings[0])


class OllamaLLM:
    """Ollama å¤§è¯­è¨€æ¨¡å‹åŒ…è£…å™¨"""
    
    def __init__(self, model_name: str = "deepseek-r1:7b"):
        self.model_name = model_name
        self.client = ollama.Client()
        
        # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
        try:
            test_response = self.client.generate(
                model=self.model_name,
                prompt="æµ‹è¯•",
                options={"num_predict": 10}
            )
            logger.info(f"Ollamaæ¨ç†æ¨¡å‹ {model_name} åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Ollamaæ¨ç†æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def generate(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """ç”Ÿæˆæ–‡æœ¬å›å¤"""
        try:
            response = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "top_k": 40
                }
            )
            return response['response']
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
    
    def generate_stream(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7):
        """æµå¼ç”Ÿæˆæ–‡æœ¬å›å¤"""
        try:
            stream = self.client.generate(
                model=self.model_name,
                prompt=prompt,
                stream=True,
                options={
                    "num_predict": max_tokens,
                    "temperature": temperature,
                    "top_p": 0.9,
                    "top_k": 40
                }
            )
            
            for chunk in stream:
                if 'response' in chunk:
                    yield chunk['response']
                    
        except Exception as e:
            logger.error(f"æµå¼æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
            yield "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"


class QuestionAnswering:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""
    
    def __init__(self, text_analyzer, llm_model_name: str = "deepseek-r1:7b"):
        self.text_analyzer = text_analyzer
        self.llm = OllamaLLM(llm_model_name)
        
        # ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
1. ä»”ç»†é˜…è¯»æä¾›çš„ç›¸å…³æ–‡æ¡£å†…å®¹
2. åŸºäºæ–‡æ¡£å†…å®¹å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜
3. å¦‚æœæ–‡æ¡£ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯¦ç»†è§£é‡Šå’Œå¼•ç”¨
4. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
5. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
6. å¯ä»¥é€‚å½“å¼•ç”¨æ–‡æ¡£ä¸­çš„å…·ä½“å†…å®¹å’ŒåŸæ–‡
7. ä¿æŒå®¢è§‚å’Œä¸“ä¸šçš„è¯­è°ƒ
8. å¯¹äºä½›å­¦ã€å“²å­¦ç­‰æ·±å±‚æ¬¡é—®é¢˜ï¼Œè¦ç»“åˆæ–‡æ¡£å†…å®¹ç»™å‡ºæœ‰æ·±åº¦çš„å›ç­”

ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£å†…å®¹è¯¦ç»†å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"""
    
    def extract_search_keywords(self, question: str) -> str:
        """ä»ç”¨æˆ·é—®é¢˜ä¸­æå–æ£€ç´¢å…³é”®è¯"""
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„å…³é”®è¯æå–é€»è¾‘
        # ç›®å‰é‡‡ç”¨ç®€å•çš„æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨é—®é¢˜ä½œä¸ºæ£€ç´¢è¯
        
        # ç§»é™¤å¸¸è§çš„ç–‘é—®è¯
        stop_words = ["ä»€ä¹ˆ", "æ€ä¹ˆ", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "æ˜¯å¦", "èƒ½å¦", "å¯ä»¥", "è¯·é—®", "ï¼Ÿ", "?"]
        
        keywords = question
        for word in stop_words:
            keywords = keywords.replace(word, "")
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        keywords = " ".join(keywords.split())
        
        # å¦‚æœå…³é”®è¯å¤ªçŸ­ï¼Œä½¿ç”¨åŸå§‹é—®é¢˜
        if len(keywords.strip()) < 2:
            keywords = question
            
        logger.info(f"ä»é—®é¢˜ '{question}' æå–å…³é”®è¯: '{keywords}'")
        return keywords
    
    def get_relevant_context(self, question: str, max_results: int = 10) -> List[Dict[str, Any]]:
        """è·å–ä¸é—®é¢˜ç›¸å…³çš„æ–‡æ¡£ä¸Šä¸‹æ–‡"""
        # æå–æ£€ç´¢å…³é”®è¯
        search_keywords = self.extract_search_keywords(question)
        
        # åœ¨å‘é‡åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨åˆ†æ•°é˜ˆå€¼ç­›é€‰
        results = self.text_analyzer.search(
            query=search_keywords,
            limit=50,  # å¢åŠ æ£€ç´¢èŒƒå›´
            score_threshold=0.5,  # ä½¿ç”¨0.5ä½œä¸ºåˆ†æ•°é˜ˆå€¼
            max_results=max_results  # é™åˆ¶æœ€ç»ˆè¿”å›æ•°é‡
        )
        
        logger.info(f"æ£€ç´¢åˆ° {len(results)} ä¸ªé«˜è´¨é‡ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼ˆåˆ†æ•°â‰¥0.5ï¼‰")
        return results
    
    def format_context(self, context_results: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        if not context_results:
            return "æš‚æ— ç›¸å…³æ–‡æ¡£å†…å®¹ã€‚"
        
        formatted_context = []
        for i, result in enumerate(context_results, 1):
            content = result['content']
            filename = result['metadata'].get('filename', 'æœªçŸ¥æ–‡æ¡£')
            category = result['metadata'].get('category', 'æœªåˆ†ç±»')
            score = result['score']
            
            context_piece = f"""
æ–‡æ¡£ {i}ï¼š
æ¥æºï¼š{filename} ({category})
ç›¸ä¼¼åº¦ï¼š{score:.3f}
å†…å®¹ï¼š{content}
"""
            formatted_context.append(context_piece)
        
        return "\n".join(formatted_context)
    
    def answer_question(self, question: str, max_results: int = 10) -> Dict[str, Any]:
        """å›ç­”ç”¨æˆ·é—®é¢˜"""
        try:
            logger.info(f"æ”¶åˆ°ç”¨æˆ·é—®é¢˜: {question}")
            
            # 1. è·å–ç›¸å…³ä¸Šä¸‹æ–‡
            context_results = self.get_relevant_context(question, max_results)
            
            if not context_results:
                return {
                    'answer': "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„é«˜è´¨é‡ä¿¡æ¯ï¼ˆç›¸ä¼¼åº¦â‰¥0.5ï¼‰ã€‚è¯·å°è¯•æ¢ä¸ªé—®æ³•æˆ–æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£å·²è¢«ç´¢å¼•ã€‚",
                    'sources': [],
                    'confidence': 0.0
                }
            
            # 2. æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
            formatted_context = self.format_context(context_results)
            
            # 3. æ„å»ºå®Œæ•´æç¤ºè¯
            full_prompt = self.system_prompt.format(
                context=formatted_context,
                question=question
            )
            
            # 4. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
            logger.info("æ­£åœ¨ç”Ÿæˆå›ç­”...")
            answer = self.llm.generate(
                prompt=full_prompt,
                max_tokens=800,
                temperature=0.3  # è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„å›ç­”
            )
            
            # æ¸…ç†å›ç­”å†…å®¹ï¼Œå»é™¤æ€è€ƒè¿‡ç¨‹æ ‡è®°
            answer = self._clean_answer(answer)
            
            # 5. è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºæ£€ç´¢ç»“æœçš„å¹³å‡ç›¸ä¼¼åº¦ï¼‰
            avg_score = sum([r['score'] for r in context_results]) / len(context_results)
            confidence = min(avg_score * 2, 1.0)  # ç®€å•çš„ç½®ä¿¡åº¦è®¡ç®—
            
            # 6. æ•´ç†è¿”å›ç»“æœ
            sources = []
            for result in context_results:
                sources.append({
                    'filename': result['metadata'].get('filename', 'æœªçŸ¥'),
                    'category': result['metadata'].get('category', 'æœªåˆ†ç±»'),
                    'score': result['score'],
                    'content_preview': result['content'][:100] + "..."
                })
            
            logger.info("é—®ç­”å®Œæˆ")
            return {
                'answer': answer.strip(),
                'sources': sources,
                'confidence': confidence,
                'context_count': len(context_results)
            }
            
        except Exception as e:
            logger.error(f"é—®ç­”è¿‡ç¨‹å‡ºé”™: {e}")
            return {
                'answer': f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}",
                'sources': [],
                'confidence': 0.0
            }
    
    def _clean_answer(self, answer: str) -> str:
        """æ¸…ç†AIå›ç­”ï¼Œå»é™¤æ€è€ƒè¿‡ç¨‹ç­‰ä¸éœ€è¦çš„å†…å®¹ï¼Œä½†ä¿æŒmarkdownæ ¼å¼"""
        # å»é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹
        import re
        
        # ç§»é™¤ <think>...</think> åŒ…å›´çš„å†…å®¹
        answer = re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL)
        
        # ç§»é™¤å…¶ä»–å¯èƒ½çš„æ ‡è®°
        answer = re.sub(r'<.*?>', '', answer)
        
        # ä¿ç•™åŸæœ‰çš„æ¢è¡Œç»“æ„ï¼Œåªæ¸…ç†é¦–å°¾ç©ºç™½å’Œè¿‡å¤šçš„è¿ç»­ç©ºè¡Œ
        # å°†å¤šä¸ªè¿ç»­çš„ç©ºè¡Œæ›¿æ¢ä¸ºæœ€å¤šä¸¤ä¸ªç©ºè¡Œ
        answer = re.sub(r'\n\s*\n\s*\n+', '\n\n', answer)
        
        # æ¸…ç†é¦–å°¾ç©ºç™½
        answer = answer.strip()
        
        return answer
    
    def interactive_qa(self):
        """äº¤äº’å¼é—®ç­”ç•Œé¢"""
        print("\n" + "=" * 60)
        print("ğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")
        print("=" * 60)
        print("æç¤ºï¼šè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¸®åŠ©")
        print()
        
        while True:
            try:
                question = input("ğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                
                if not question:
                    continue
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if question.lower() in ['help', 'å¸®åŠ©']:
                    self.show_help()
                    continue
                
                print("\nğŸ” æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...")
                
                # å›ç­”é—®é¢˜
                result = self.answer_question(question)
                
                # æ˜¾ç¤ºå›ç­”
                print(f"\nğŸ¤– å›ç­”ï¼š")
                print("-" * 40)
                print(result['answer'])
                
                # æ˜¾ç¤ºä¿¡æ¯æ¥æº
                if result['sources']:
                    print(f"\nğŸ“š ä¿¡æ¯æ¥æº (ç½®ä¿¡åº¦: {result['confidence']:.2f})ï¼š")
                    for i, source in enumerate(result['sources'], 1):
                        print(f"{i}. {source['filename']} ({source['category']}) - ç›¸ä¼¼åº¦: {source['score']:.3f}")
                        print(f"   å†…å®¹é¢„è§ˆ: {source['content_preview']}")
                
                print("\n" + "-" * 60)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = """
ğŸ“– æ™ºèƒ½é—®ç­”ç³»ç»Ÿå¸®åŠ©ï¼š

ğŸ”¹ åŠŸèƒ½è¯´æ˜ï¼š
   - åŸºäºå·²ç´¢å¼•çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
   - è‡ªåŠ¨æ£€ç´¢æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
   - ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆå‡†ç¡®å›ç­”

ğŸ”¹ ä½¿ç”¨æŠ€å·§ï¼š
   - é—®é¢˜å°½é‡å…·ä½“æ˜ç¡®
   - å¯ä»¥è¯¢é—®æŠ€æœ¯ã€é¡¹ç›®ã€ä¼šè®®ç­‰ç›¸å…³å†…å®¹
   - æ”¯æŒä¸­æ–‡é—®ç­”

ğŸ”¹ ç¤ºä¾‹é—®é¢˜ï¼š
   - "å¦‚ä½•ä½¿ç”¨APIè¿›è¡Œç”¨æˆ·è®¤è¯ï¼Ÿ"
   - "é¡¹ç›®çš„æŠ€æœ¯æ¶æ„æ˜¯ä»€ä¹ˆï¼Ÿ"
   - "ä¼šè®®ä¸­è®¨è®ºäº†å“ªäº›å†³è®®ï¼Ÿ"

ğŸ”¹ å‘½ä»¤ï¼š
   - quit/exit/é€€å‡ºï¼šé€€å‡ºé—®ç­”ç³»ç»Ÿ
   - help/å¸®åŠ©ï¼šæ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
"""
        print(help_text)


class CustomMetadataExtractor:
    """è‡ªå®šä¹‰å…ƒæ•°æ®æå–å™¨ï¼Œä»æ–‡ä»¶åå’Œè·¯å¾„æå–åˆ†ç±»æ ‡ç­¾"""
    
    def __init__(self):
        # å®šä¹‰æ–‡ä»¶ç±»å‹æ˜ å°„è§„åˆ™
        self.category_patterns = {
            'æŠ€æœ¯æ–‡æ¡£': [r'tech|æŠ€æœ¯|å¼€å‘|api|sdk|ä»£ç ', r'dev|development'],
            'ç”¨æˆ·æ‰‹å†Œ': [r'manual|æ‰‹å†Œ|æŒ‡å—|guide|tutorial', r'ç”¨æˆ·|user'],
            'é¡¹ç›®èµ„æ–™': [r'project|é¡¹ç›®|éœ€æ±‚|requirement', r'spec|specification'],
            'ä¼šè®®è®°å½•': [r'meeting|ä¼šè®®|çºªè¦|è®°å½•|minutes'],
            'æŠ¥å‘Šæ–‡æ¡£': [r'report|æŠ¥å‘Š|åˆ†æ|analysis|summary'],
            'å­¦ä¹ ç¬”è®°': [r'note|ç¬”è®°|å­¦ä¹ |study|learn'],
            'å…¶ä»–': []  # é»˜è®¤åˆ†ç±»
        }
    
    def extract_metadata(self, file_path: str) -> Dict[str, Any]:
        """ä»æ–‡ä»¶è·¯å¾„å’Œåç§°æå–å…ƒæ•°æ®"""
        path_obj = Path(file_path)
        filename = path_obj.stem.lower()
        parent_dir = path_obj.parent.name.lower()
        
        # æå–åŸºæœ¬ä¿¡æ¯
        metadata = {
            'filename': path_obj.name,
            'file_path': str(file_path),
            'file_size': path_obj.stat().st_size if path_obj.exists() else 0,
            'modified_time': path_obj.stat().st_mtime if path_obj.exists() else 0,
            'created_time': datetime.now().isoformat(),
            'category': 'å…¶ä»–',  # é»˜è®¤åˆ†ç±»
            'tags': []
        }
        
        # æ ¹æ®æ–‡ä»¶åå’Œç›®å½•ååˆ¤æ–­åˆ†ç±»
        text_to_analyze = f"{filename} {parent_dir}"
        
        for category, patterns in self.category_patterns.items():
            if category == 'å…¶ä»–':
                continue
            
            for pattern in patterns:
                if re.search(pattern, text_to_analyze, re.IGNORECASE):
                    metadata['category'] = category
                    break
            
            if metadata['category'] != 'å…¶ä»–':
                break
        
        # æå–å¯èƒ½çš„æ ‡ç­¾
        tags = set()
        if 'æŠ€æœ¯' in text_to_analyze or 'tech' in text_to_analyze:
            tags.add('æŠ€æœ¯')
        if 'æ–‡æ¡£' in text_to_analyze or 'doc' in text_to_analyze:
            tags.add('æ–‡æ¡£')
        if 'é¡¹ç›®' in text_to_analyze or 'project' in text_to_analyze:
            tags.add('é¡¹ç›®')
        
        metadata['tags'] = list(tags)
        
        return metadata


class ChineseTextAnalyzer:
    """ä¸­æ–‡æ–‡æœ¬åˆ†æå’Œæ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(
        self,
        model_name: str = "quentinz/bge-large-zh-v1.5",
        use_ollama: bool = True,
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333,
        collection_name: str = "wx_notes",
        chunk_similarity_threshold: float = 0.8
    ):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†æå™¨
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°
            use_ollama: æ˜¯å¦ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹
            qdrant_host: QdrantæœåŠ¡å™¨åœ°å€
            qdrant_port: QdrantæœåŠ¡å™¨ç«¯å£
            collection_name: å‘é‡é›†åˆåç§°
            chunk_similarity_threshold: è¯­ä¹‰åˆ†å—ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.model_name = model_name
        self.use_ollama = use_ollama
        self.collection_name = collection_name
        self.chunk_similarity_threshold = chunk_similarity_threshold
        
        # åˆå§‹åŒ–æ–‡æ¡£ç¼“å­˜
        self.cache_file = Path("document_cache.json")
        self.document_cache = self._load_cache()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.metadata_extractor = CustomMetadataExtractor()
        self._init_embedding_model()
        self._init_qdrant_client(qdrant_host, qdrant_port)
        self._init_semantic_chunker()
        
        # åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ
        self.qa_system = None
        
        logger.info(f"æ–‡æœ¬åˆ†æå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
    
    def init_qa_system(self, llm_model_name: str = "deepseek-r1:7b"):
        """åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ"""
        try:
            self.qa_system = QuestionAnswering(self, llm_model_name)
            logger.info(f"é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æ¨¡å‹: {llm_model_name}")
            return True
        except Exception as e:
            logger.error(f"é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def _load_cache(self) -> Dict[str, Dict]:
        """åŠ è½½æ–‡æ¡£ç¼“å­˜"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache = json.load(f)
                logger.info(f"åŠ è½½æ–‡æ¡£ç¼“å­˜ï¼ŒåŒ…å« {len(cache)} ä¸ªæ–‡æ¡£è®°å½•")
                return cache
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        return {}
    
    def _save_cache(self):
        """ä¿å­˜æ–‡æ¡£ç¼“å­˜"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.document_cache, f, ensure_ascii=False, indent=2)
            logger.info(f"ä¿å­˜æ–‡æ¡£ç¼“å­˜ï¼ŒåŒ…å« {len(self.document_cache)} ä¸ªæ–‡æ¡£è®°å½•")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
    
    def _get_file_hash(self, file_path: str, content: str = None) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆåŸºäºè·¯å¾„ã€å¤§å°ã€ä¿®æ”¹æ—¶é—´å’Œå†…å®¹ï¼‰"""
        path_obj = Path(file_path)
        if not path_obj.exists():
            return ""
        
        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„ã€å¤§å°ã€ä¿®æ”¹æ—¶é—´ç”ŸæˆåŸºç¡€å“ˆå¸Œ
        file_stat = path_obj.stat()
        hash_components = [
            str(file_path),
            str(file_stat.st_size),
            str(file_stat.st_mtime)
        ]
        
        # å¦‚æœæä¾›äº†å†…å®¹ï¼Œä¹ŸåŠ å…¥å“ˆå¸Œè®¡ç®—
        if content:
            hash_components.append(content[:1000])  # ä½¿ç”¨å‰1000å­—ç¬¦
        
        hash_string = '|'.join(hash_components)
        return hashlib.md5(hash_string.encode('utf-8')).hexdigest()
    
    def _is_document_cached(self, file_path: str, content: str = None) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ç¼“å­˜ä¸”æœªå˜åŒ–"""
        file_hash = self._get_file_hash(file_path, content)
        if not file_hash:
            return False
        
        cache_key = str(file_path)
        if cache_key in self.document_cache:
            cached_info = self.document_cache[cache_key]
            return cached_info.get('file_hash') == file_hash
        
        return False
    
    def _update_document_cache(self, file_path: str, content: str, chunk_count: int):
        """æ›´æ–°æ–‡æ¡£ç¼“å­˜ä¿¡æ¯"""
        file_hash = self._get_file_hash(file_path, content)
        cache_key = str(file_path)
        
        self.document_cache[cache_key] = {
            'file_hash': file_hash,
            'chunk_count': chunk_count,
            'last_processed': datetime.now().isoformat(),
            'collection_name': self.collection_name
        }
        
        # ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶
        self._save_cache()
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            if self.use_ollama:
                self.embedding_model = OllamaEmbedding(self.model_name)
                logger.info(f"ä½¿ç”¨Ollamaæœ¬åœ°åµŒå…¥æ¨¡å‹: {self.model_name}")
            else:
                self.embedding_model = SentenceTransformer(self.model_name)
                logger.info(f"ä½¿ç”¨SentenceTransformersåµŒå…¥æ¨¡å‹: {self.model_name}")
        except Exception as e:
            logger.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _init_qdrant_client(self, host: str, port: int):
        """åˆå§‹åŒ–Qdrantå®¢æˆ·ç«¯"""
        try:
            # è®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…åœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶è¶…æ—¶
            self.qdrant_client = QdrantClient(
                host=host, 
                port=port,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            logger.info(f"Qdrantå®¢æˆ·ç«¯è¿æ¥æˆåŠŸ: {host}:{port}")
            
            # åˆ›å»ºé›†åˆï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            self._create_collection_if_not_exists()
            
        except Exception as e:
            logger.warning(f"æ— æ³•è¿æ¥åˆ°QdrantæœåŠ¡å™¨ï¼Œå°†ä½¿ç”¨å†…å­˜æ¨¡å¼: {e}")
            # å†…å­˜æ¨¡å¼ä¹Ÿè®¾ç½®è¶…æ—¶
            self.qdrant_client = QdrantClient(
                ":memory:",
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            self._create_collection_if_not_exists()
    
    def _create_collection_if_not_exists(self):
        """åˆ›å»ºQdranté›†åˆï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        try:
            collections = self.qdrant_client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if self.collection_name not in collection_names:
                # è·å–åµŒå…¥ç»´åº¦
                sample_embedding = self.embedding_model.encode("test")
                vector_size = len(sample_embedding)
                
                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name}, å‘é‡ç»´åº¦: {vector_size}")
            else:
                logger.info(f"é›†åˆ {self.collection_name} å·²å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"åˆ›å»ºé›†åˆå¤±è´¥: {e}")
            raise
    
    def _init_semantic_chunker(self):
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨"""
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
        )
        logger.info("æ–‡æœ¬åˆ†å—å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def load_documents(self, file_paths: Union[str, List[str]]) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£æ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„ï¼ˆå•ä¸ªè·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨ï¼‰
            
        Returns:
            åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨
        """
        if isinstance(file_paths, str):
            file_paths = [file_paths]
        
        documents = []
        
        for file_path in file_paths:
            try:
                # ä½¿ç”¨TextLoaderåŠ è½½æ–‡ä»¶
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
                
                # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ è‡ªå®šä¹‰å…ƒæ•°æ®
                for doc in docs:
                    custom_metadata = self.metadata_extractor.extract_metadata(file_path)
                    doc.metadata.update(custom_metadata)
                
                documents.extend(docs)
                logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {file_path}")
                
            except Exception as e:
                logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
                continue
        
        logger.info(f"å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        return documents
    
    def chunk_documents(self, documents: List[Document]) -> List[Document]:
        """
        ä½¿ç”¨æ–‡æœ¬åˆ†å—å™¨å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        chunked_documents = []
        
        for doc in documents:
            try:
                # ä½¿ç”¨æ–‡æœ¬åˆ†å—å™¨åˆ†å—
                chunks = self.text_splitter.split_documents([doc])
                
                # ä¸ºæ¯ä¸ªchunkæ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
                for i, chunk in enumerate(chunks):
                    chunk.metadata['chunk_id'] = i
                    chunk.metadata['total_chunks'] = len(chunks)
                    chunk.metadata['chunk_size'] = len(chunk.page_content)
                
                chunked_documents.extend(chunks)
                logger.info(f"æ–‡æ¡£ {doc.metadata.get('filename', 'unknown')} åˆ†æˆ {len(chunks)} å—")
                
            except Exception as e:
                logger.error(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {e}")
                continue
        
        logger.info(f"å…±ç”Ÿæˆ {len(chunked_documents)} ä¸ªæ–‡æ¡£å—")
        return chunked_documents
    
    def embed_and_store(self, documents: List[Document]) -> bool:
        """
        å¯¹æ–‡æ¡£è¿›è¡ŒåµŒå…¥å¹¶å­˜å‚¨åˆ°Qdrant
        
        Args:
            documents: è¦å­˜å‚¨çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            æ˜¯å¦æˆåŠŸå­˜å‚¨
        """
        try:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†å¤ªå¤šæ•°æ®å¯¼è‡´è¶…æ—¶
            batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªæ–‡æ¡£
            total_docs = len(documents)
            
            for batch_start in range(0, total_docs, batch_size):
                batch_end = min(batch_start + batch_size, total_docs)
                batch_docs = documents[batch_start:batch_end]
                
                points = []
                
                for i, doc in enumerate(batch_docs):
                    # ç”ŸæˆåµŒå…¥å‘é‡
                    embedding = self.embedding_model.encode(doc.page_content)
                    
                    # åˆ›å»ºç‚¹æ•°æ®ï¼Œä½¿ç”¨å…¨å±€ç´¢å¼•ä½œä¸ºID
                    point = PointStruct(
                        id=batch_start + i,
                        vector=embedding.tolist(),
                        payload={
                            'content': doc.page_content,
                            'metadata': doc.metadata
                        }
                    )
                    points.append(point)
                
                # æ‰¹é‡æ’å…¥åˆ°Qdrant
                self.qdrant_client.upsert(
                    collection_name=self.collection_name,
                    points=points,
                    wait=True  # ç­‰å¾…æ“ä½œå®Œæˆ
                )
                
                logger.info(f"æˆåŠŸå­˜å‚¨æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(total_docs-1)//batch_size + 1}: {len(points)} ä¸ªæ–‡æ¡£å‘é‡")
            
            logger.info(f"æˆåŠŸå­˜å‚¨æ‰€æœ‰ {total_docs} ä¸ªæ–‡æ¡£å‘é‡")
            return True
            
        except Exception as e:
            logger.error(f"å­˜å‚¨æ–‡æ¡£å‘é‡å¤±è´¥: {e}")
            return False
    
    def search(
        self,
        query: str,
        limit: int = 50,  # å¢åŠ é»˜è®¤limitä»¥æ”¯æŒæ›´å¤šç»“æœ
        category_filter: Optional[str] = None,
        tags_filter: Optional[List[str]] = None,
        score_threshold: float = 0.5,  # æé«˜é»˜è®¤é˜ˆå€¼åˆ°0.5
        max_results: Optional[int] = None  # æ–°å¢ï¼šæœ€å¤§ç»“æœæ•°é™åˆ¶
    ) -> List[Dict[str, Any]]:
        """
        æ™ºèƒ½æ£€ç´¢æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            limit: æ£€ç´¢æ—¶çš„æ•°é‡é™åˆ¶ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
            category_filter: æŒ‰åˆ†ç±»ç­›é€‰
            tags_filter: æŒ‰æ ‡ç­¾ç­›é€‰
            score_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.5ï¼‰
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆæŒ‰åˆ†æ•°é˜ˆå€¼ç­›é€‰ï¼‰
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(query)
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            filter_conditions = []
            
            if category_filter:
                filter_conditions.append(
                    FieldCondition(
                        key="metadata.category",  # ä½¿ç”¨åµŒå¥—å­—æ®µè·¯å¾„
                        match=MatchValue(value=category_filter)
                    )
                )
            
            if tags_filter:
                for tag in tags_filter:
                    filter_conditions.append(
                        FieldCondition(
                            key="metadata.tags",  # ä½¿ç”¨åµŒå¥—å­—æ®µè·¯å¾„
                            match=MatchValue(value=tag)
                        )
                    )
            
            # æ‰§è¡Œæœç´¢
            search_filter = Filter(must=filter_conditions) if filter_conditions else None
            
            search_results = self.qdrant_client.query_points(
                collection_name=self.collection_name,
                query=query_embedding.tolist(),
                limit=limit,
                score_threshold=0.0,  # åœ¨æ•°æ®åº“å±‚é¢ä¸è¿‡æ»¤ï¼Œè®©åº”ç”¨å±‚å¤„ç†
                query_filter=search_filter
            ).points
            
            # æ ¼å¼åŒ–ç»“æœ
            results = []
            for result in search_results:
                # åªä¿ç•™åˆ†æ•°è¶…è¿‡é˜ˆå€¼çš„ç»“æœ
                if result.score >= score_threshold:
                    # ä»payloadä¸­æå–å†…å®¹å’Œå…ƒæ•°æ®
                    content = result.payload.get('content', '')
                    metadata = result.payload.get('metadata', {})
                    
                    results.append({
                        'content': content,
                        'metadata': {
                            'filename': metadata.get('filename', ''),
                            'category': metadata.get('category', ''),
                            'upload_time': metadata.get('created_time', ''),
                            'file_size': metadata.get('file_size', 0),
                            'chunk_index': metadata.get('chunk_id', 0),
                            'total_chunks': metadata.get('total_chunks', 1)
                        },
                        'score': result.score,
                        'id': result.id
                    })
            
            # å¦‚æœè®¾ç½®äº†æœ€å¤§ç»“æœæ•°é™åˆ¶ï¼Œåˆ™æˆªå–
            if max_results and len(results) > max_results:
                results = results[:max_results]
            
            logger.info(f"æ£€ç´¢åˆ° {len(search_results)} ä¸ªç»“æœï¼Œç­›é€‰åä¿ç•™ {len(results)} ä¸ªé«˜è´¨é‡ç»“æœï¼ˆåˆ†æ•°â‰¥{score_threshold}ï¼‰")
            return results
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """è·å–é›†åˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            
            stats = {
                'total_points': collection_info.points_count,
                'vector_size': collection_info.config.params.vectors.size,
                'distance_metric': collection_info.config.params.vectors.distance,
                'collection_name': self.collection_name
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def process_directory(
        self,
        directory_path: str,
        file_pattern: str = "*",  # æ”¹ä¸ºåŒ¹é…æ‰€æœ‰æ–‡ä»¶
        force_refresh: bool = False
    ) -> bool:
        """
        å¤„ç†æ•´ä¸ªç›®å½•ä¸­çš„æ–‡æœ¬æ–‡ä»¶
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼ï¼Œé»˜è®¤ä¸º"*"åŒ¹é…æ‰€æœ‰æ–‡ä»¶
            force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            directory = Path(directory_path)
            if not directory.exists():
                logger.error(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
                return False
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
            all_files = list(directory.glob(file_pattern))
            # è¿‡æ»¤æ‰éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
            txt_files = [f for f in all_files if not f.name.startswith('.') and not f.name.startswith('~')]
            
            if not txt_files:
                logger.warning(f"ç›®å½• {directory_path} ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶")
                return False
            
            file_paths = [str(f) for f in txt_files]
            logger.info(f"æ‰¾åˆ° {len(file_paths)} ä¸ªæ–‡ä»¶: {file_paths}")
            
            # æ£€æŸ¥å“ªäº›æ–‡ä»¶éœ€è¦é‡æ–°å¤„ç†
            files_to_process = []
            cached_files = []
            
            if not force_refresh:
                for file_path in file_paths:
                    try:
                        # è¯»å–æ–‡ä»¶å†…å®¹ç”¨äºç¼“å­˜æ£€æŸ¥
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        if self._is_document_cached(file_path, content):
                            cached_files.append(file_path)
                            logger.info(f"æ–‡æ¡£ {Path(file_path).name} æœªå˜åŒ–ï¼Œè·³è¿‡å¤„ç†")
                        else:
                            files_to_process.append(file_path)
                    except Exception as e:
                        logger.warning(f"æ£€æŸ¥æ–‡ä»¶ç¼“å­˜å¤±è´¥ {file_path}: {e}")
                        files_to_process.append(file_path)
            else:
                files_to_process = file_paths
                logger.info("å¼ºåˆ¶åˆ·æ–°æ¨¡å¼ï¼Œå°†é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶")
            
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶éœ€è¦å¤„ç†ï¼Œç›´æ¥è¿”å›æˆåŠŸ
            if not files_to_process:
                logger.info("æ‰€æœ‰æ–‡æ¡£éƒ½å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼Œæ— éœ€é‡æ–°å¤„ç†")
                return True
            
            logger.info(f"éœ€è¦å¤„ç† {len(files_to_process)} ä¸ªæ–‡ä»¶ï¼Œ{len(cached_files)} ä¸ªæ–‡ä»¶ä½¿ç”¨ç¼“å­˜")
            
            # åŠ è½½éœ€è¦å¤„ç†çš„æ–‡æ¡£
            documents = self.load_documents(files_to_process)
            if not documents:
                logger.error("æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£")
                return False
            
            # åˆ†å—å¤„ç†
            chunked_docs = self.chunk_documents(documents)
            if not chunked_docs:
                logger.error("æ–‡æ¡£åˆ†å—å¤±è´¥")
                return False
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            success = self.embed_and_store(chunked_docs)
            if success:
                # æ›´æ–°ç¼“å­˜ä¿¡æ¯
                for doc in documents:
                    file_path = doc.metadata['file_path']
                    content = doc.page_content
                    # è®¡ç®—è¯¥æ–‡æ¡£çš„åˆ†å—æ•°
                    doc_chunks = [chunk for chunk in chunked_docs 
                                if chunk.metadata.get('file_path') == file_path]
                    self._update_document_cache(file_path, content, len(doc_chunks))
                
                logger.info("ç›®å½•å¤„ç†å®Œæˆ")
                return True
            else:
                logger.error("å‘é‡å­˜å‚¨å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"å¤„ç†ç›®å½•å¤±è´¥: {e}")
            return False
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        cache_info = {
            'total_cached_files': len(self.document_cache),
            'cache_file_path': str(self.cache_file),
            'cache_exists': self.cache_file.exists(),
            'files': {}
        }
        
        for file_path, info in self.document_cache.items():
            filename = Path(file_path).name
            cache_info['files'][filename] = {
                'chunk_count': info.get('chunk_count', 0),
                'last_processed': info.get('last_processed', 'Unknown'),
                'collection': info.get('collection_name', 'Unknown')
            }
        
        return cache_info
    
    def clear_cache(self):
        """æ¸…ç©ºæ–‡æ¡£ç¼“å­˜"""
        self.document_cache = {}
        if self.cache_file.exists():
            self.cache_file.unlink()
        logger.info("æ–‡æ¡£ç¼“å­˜å·²æ¸…ç©º")
    
    def process_single_document(self, content: str, filename: str, category: str = "æœªåˆ†ç±»") -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ–‡æ¡£å†…å®¹å¹¶æ·»åŠ åˆ°å‘é‡åº“"""
        try:
            # åˆ›å»ºDocumentå¯¹è±¡
            doc = Document(
                page_content=content,
                metadata={
                    'filename': filename,
                    'category': category,
                    'upload_time': datetime.now().isoformat(),
                    'file_size': len(content.encode('utf-8'))
                }
            )
            
            # åˆ†å—å¤„ç†
            chunks = self.text_splitter.split_documents([doc])
            
            if not chunks:
                return {
                    'success': False,
                    'error': f'æ–‡æ¡£ {filename} åˆ†å—å¤±è´¥'
                }
            
            # ç”ŸæˆåµŒå…¥å‘é‡
            texts = [chunk.page_content for chunk in chunks]
            embeddings = self.embedding_model.encode(texts)
            
            # å‡†å¤‡å‘é‡æ•°æ®
            points = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                # ç”Ÿæˆå®‰å…¨çš„UUIDä½œä¸ºç‚¹ID
                point_id = str(uuid.uuid4())
                
                points.append(PointStruct(
                    id=point_id,
                    vector=embedding.tolist(),
                    payload={
                        "text": chunk.page_content,
                        "filename": chunk.metadata["filename"],
                        "category": chunk.metadata["category"],
                        "upload_time": chunk.metadata["upload_time"],
                        "file_size": chunk.metadata["file_size"],
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    }
                ))
            
            # ä¸Šä¼ åˆ°å‘é‡åº“
            self.qdrant_client.upsert(
                collection_name=self.collection_name,
                points=points,
                wait=True  # ç­‰å¾…æ“ä½œå®Œæˆ
            )
            
            # æ›´æ–°ç¼“å­˜
            cache_key = f"{filename}_{category}"
            self.document_cache[cache_key] = {
                'filename': filename,
                'category': category,
                'chunk_count': len(chunks),
                'last_processed': datetime.now().isoformat(),
                'collection_name': self.collection_name,
                'file_hash': self._get_file_hash("", content),
                'file_size': len(content.encode('utf-8'))
            }
            self._save_cache()
            
            logger.info(f"æˆåŠŸå¤„ç†æ–‡æ¡£ {filename}ï¼Œæ·»åŠ  {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            return {
                'success': True,
                'chunks_added': len(chunks),
                'message': f'æˆåŠŸå¤„ç†æ–‡æ¡£ {filename}'
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£ {filename} æ—¶å‡ºé”™: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_document_list(self) -> List[Dict[str, Any]]:
        """è·å–å·²å¯¼å…¥çš„æ–‡æ¡£åˆ—è¡¨"""
        try:
            documents = []
            
            # ä»ç¼“å­˜ä¸­è·å–æ–‡æ¡£ä¿¡æ¯
            for cache_key, info in self.document_cache.items():
                documents.append({
                    'filename': info.get('filename', 'Unknown'),
                    'category': info.get('category', 'æœªåˆ†ç±»'),
                    'chunks': info.get('chunk_count', 0),
                    'upload_time': info.get('last_processed', ''),
                    'file_size': info.get('file_size', 0)
                })
            
            # æŒ‰ä¸Šä¼ æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
            documents.sort(key=lambda x: x.get('upload_time', ''), reverse=True)
            
            return documents
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return []
    
    def delete_document(self, filename: str, category: str = None) -> Dict[str, Any]:
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        try:
            deleted_chunks = 0
            
            # æ„å»ºè¿‡æ»¤æ¡ä»¶
            must_conditions = [
                FieldCondition(key="filename", match=MatchValue(value=filename))
            ]
            
            if category:
                must_conditions.append(
                    FieldCondition(key="category", match=MatchValue(value=category))
                )
            
            filter_condition = Filter(must=must_conditions)
            
            # å…ˆæŸ¥è¯¢è¦åˆ é™¤çš„ç‚¹
            search_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter=filter_condition,
                limit=1000  # å‡è®¾å•ä¸ªæ–‡æ¡£ä¸ä¼šè¶…è¿‡1000ä¸ªç‰‡æ®µ
            )
            
            if search_result[0]:  # å¦‚æœæœ‰æ‰¾åˆ°çš„ç‚¹
                # åˆ é™¤å‘é‡æ•°æ®
                point_ids = [point.id for point in search_result[0]]
                self.qdrant_client.delete(
                    collection_name=self.collection_name,
                    points_selector=point_ids
                )
                deleted_chunks = len(point_ids)
                
                # ä»ç¼“å­˜ä¸­åˆ é™¤
                cache_key = f"{filename}_{category}" if category else None
                if cache_key and cache_key in self.document_cache:
                    del self.document_cache[cache_key]
                else:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†ç±»ï¼Œåˆ é™¤æ‰€æœ‰åŒ¹é…æ–‡ä»¶åçš„æ¡ç›®
                    keys_to_delete = [key for key in self.document_cache.keys() 
                                    if self.document_cache[key].get('filename') == filename]
                    for key in keys_to_delete:
                        del self.document_cache[key]
                
                self._save_cache()
                
                logger.info(f"æˆåŠŸåˆ é™¤æ–‡æ¡£ {filename}ï¼Œåˆ é™¤äº† {deleted_chunks} ä¸ªç‰‡æ®µ")
                
                return {
                    'success': True,
                    'deleted_chunks': deleted_chunks,
                    'message': f'æˆåŠŸåˆ é™¤æ–‡æ¡£ {filename}'
                }
            else:
                return {
                    'success': False,
                    'error': f'æœªæ‰¾åˆ°æ–‡æ¡£ {filename}'
                }
                
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£ {filename} æ—¶å‡ºé”™: {e}")
            return {
                'success': False,
                'error': str(e)
            }


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºä½¿ç”¨"""
    
    # åˆå§‹åŒ–æ–‡æœ¬åˆ†æå™¨ï¼Œä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹
    analyzer = ChineseTextAnalyzer(
        model_name="quentinz/bge-large-zh-v1.5",
        use_ollama=True,
        qdrant_host="localhost",
        qdrant_port=6333,
        collection_name="wx_notes",
        chunk_similarity_threshold=0.8
    )
    
    # ç¤ºä¾‹ï¼šå¤„ç†å½“å‰ç›®å½•ä¸­çš„txtæ–‡ä»¶
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„
    notes_dir = os.path.join(os.path.dirname(__file__), "notes")
    current_dir = notes_dir if os.path.exists(notes_dir) else "."
    
    print("=" * 60)
    print("æ™ºèƒ½æ–‡æœ¬åˆ†æå’Œæ£€ç´¢ç³»ç»Ÿï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰")
    print("=" * 60)
    
    # æ˜¾ç¤ºç¼“å­˜ä¿¡æ¯
    cache_info = analyzer.get_cache_info()
    print(f"ğŸ“¦ ç¼“å­˜çŠ¶æ€: {cache_info['total_cached_files']} ä¸ªæ–‡ä»¶å·²ç¼“å­˜")
    if cache_info['files']:
        print("   ç¼“å­˜æ–‡ä»¶è¯¦æƒ…:")
        for filename, info in cache_info['files'].items():
            print(f"   - {filename}: {info['chunk_count']} å—, å¤„ç†æ—¶é—´: {info['last_processed'][:19]}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶éœ€è¦å¤„ç†ï¼ˆæ‰€æœ‰æ–‡ä»¶ï¼Œä¸ä»…ä»…æ˜¯txtï¼‰
    all_files = list(Path(current_dir).glob("*"))
    # è¿‡æ»¤æ‰éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
    text_files = [f for f in all_files if f.is_file() and not f.name.startswith('.') and not f.name.startswith('~')]
    
    if text_files:
        print(f"\nå‘ç° {len(text_files)} ä¸ªæ–‡ä»¶ï¼Œå¼€å§‹æ™ºèƒ½å¤„ç†...")
        
        # å¤„ç†æ–‡ä»¶ï¼ˆå¯ç”¨ç¼“å­˜ä¼˜åŒ–ï¼Œä½¿ç”¨æ‰€æœ‰æ–‡ä»¶æ¨¡å¼ï¼‰
        success = analyzer.process_directory(current_dir, file_pattern="*", force_refresh=False)
        
        if success:
            print("âœ… æ–‡ä»¶å¤„ç†å®Œæˆï¼")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = analyzer.get_collection_stats()
            print(f"ğŸ“Š é›†åˆç»Ÿè®¡: {stats}")
            
            # æ˜¾ç¤ºæ›´æ–°åçš„ç¼“å­˜ä¿¡æ¯
            updated_cache_info = analyzer.get_cache_info()
            print(f"ğŸ“¦ æ›´æ–°åç¼“å­˜: {updated_cache_info['total_cached_files']} ä¸ªæ–‡ä»¶å·²ç¼“å­˜")
            
            # ç¤ºä¾‹æ£€ç´¢
            print("\nğŸ” æ£€ç´¢ç¤ºä¾‹:")
            
            # æ™®é€šæ£€ç´¢
            results = analyzer.search("æŠ€æœ¯æ–‡æ¡£", limit=3)
            print(f"æ£€ç´¢'æŠ€æœ¯æ–‡æ¡£'ï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
            
            for i, result in enumerate(results, 1):
                print(f"{i}. ç›¸ä¼¼åº¦: {result['score']:.3f}")
                print(f"   åˆ†ç±»: {result['metadata'].get('category', 'unknown')}")
                print(f"   æ–‡ä»¶: {result['metadata'].get('filename', 'unknown')}")
                print(f"   å†…å®¹é¢„è§ˆ: {result['content'][:100]}...")
                print()
            
            # æŒ‰åˆ†ç±»ç­›é€‰æ£€ç´¢
            results_filtered = analyzer.search(
                "å¼€å‘",
                limit=2,
                category_filter="æŠ€æœ¯æ–‡æ¡£"
            )
            print(f"ç­›é€‰'æŠ€æœ¯æ–‡æ¡£'ç±»åˆ«ä¸­å…³äº'å¼€å‘'çš„å†…å®¹ï¼Œæ‰¾åˆ° {len(results_filtered)} ä¸ªç»“æœ")
            
            # åˆå§‹åŒ–å¹¶æ¼”ç¤ºé—®ç­”åŠŸèƒ½
            print("\nğŸ¤– åˆå§‹åŒ–æ™ºèƒ½é—®ç­”ç³»ç»Ÿ...")
            qa_success = analyzer.init_qa_system("deepseek-r1:7b")
            
            if qa_success:
                print("âœ… é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸï¼")
                
                # æ¼”ç¤ºé—®ç­”åŠŸèƒ½
                print("\nğŸ“ é—®ç­”åŠŸèƒ½æ¼”ç¤º:")
                demo_questions = [
                    "æŠ€æœ¯æ¶æ„æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "å¦‚ä½•è¿›è¡Œç”¨æˆ·è®¤è¯ï¼Ÿ",
                    "é¡¹ç›®æœ‰å“ªäº›åŠŸèƒ½æ¨¡å—ï¼Ÿ"
                ]
                
                for i, question in enumerate(demo_questions, 1):
                    print(f"\n{i}. é—®é¢˜: {question}")
                    result = analyzer.qa_system.answer_question(question)
                    print(f"   å›ç­”: {result['answer'][:150]}...")
                    print(f"   ç½®ä¿¡åº¦: {result['confidence']:.2f}")
                    print(f"   å‚è€ƒæ¥æº: {len(result['sources'])} ä¸ªæ–‡æ¡£")
                
                # å¯åŠ¨äº¤äº’å¼é—®ç­”
                print("\nğŸš€ å¯åŠ¨äº¤äº’å¼é—®ç­”ç³»ç»Ÿ...")
                try:
                    analyzer.qa_system.interactive_qa()
                except KeyboardInterrupt:
                    print("\né—®ç­”ç³»ç»Ÿå·²é€€å‡º")
            else:
                print("âŒ é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
                print("è¯·ç¡®ä¿ deepseek-r1:7b æ¨¡å‹å·²å®‰è£…å¹¶å¯ç”¨")
            
        else:
            print("âŒ æ–‡ä»¶å¤„ç†å¤±è´¥")
    
    else:
        print("ğŸ“ æœªæ‰¾åˆ°txtæ–‡ä»¶ï¼Œè¯·æ·»åŠ ä¸€äº›txtæ–‡ä»¶åˆ°ç›®å½•ä¸­è¿›è¡Œæµ‹è¯•")
        print("\nå¯ä»¥åˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡ä»¶ï¼š")
        print("- tech_api.txt (æŠ€æœ¯æ–‡æ¡£)")
        print("- user_manual.txt (ç”¨æˆ·æ‰‹å†Œ)")
        print("- project_requirements.txt (é¡¹ç›®èµ„æ–™)")
        print("- meeting_notes.txt (ä¼šè®®è®°å½•)")


if __name__ == "__main__":
    main()
