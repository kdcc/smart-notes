#!/usr/bin/env python3
"""
æ™ºèƒ½é—®ç­”ç³»ç»Ÿ - ç»Ÿä¸€åº”ç”¨å…¥å£
é›†æˆWebåº”ç”¨å’Œå‘½ä»¤è¡Œå¯åŠ¨å™¨åŠŸèƒ½

æ”¯æŒçš„å¯åŠ¨æ¨¡å¼ï¼š
1. web - å¯åŠ¨Webåº”ç”¨ï¼ˆé»˜è®¤ï¼Œè‡ªåŠ¨å‘é‡é‡å»ºï¼‰
2. demo - å¯åŠ¨å‘½ä»¤è¡Œé—®ç­”æ¼”ç¤º  
3. cache - å¯åŠ¨ç¼“å­˜ç®¡ç†å·¥å…·
4. rebuild - ä»…æ‰§è¡Œå‘é‡é‡å»º
5. status - æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
6. api-only - ä»…å¯åŠ¨Web APIï¼ˆæ— æµè§ˆå™¨ç•Œé¢ï¼‰
"""

import sys
import os
import subprocess
import argparse
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•å’Œå·¥å…·ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "tools"))
sys.path.insert(0, str(project_root / "core"))

# Flaskç›¸å…³å¯¼å…¥
from flask import Flask, render_template, request, jsonify, session, Response
from text_analyzer import ChineseTextAnalyzer, QuestionAnswering
from startup import startup_vector_rebuild, VectorIndexManager
from keyword_processor import KeywordProcessor, KeywordSearchEngine
from datetime import datetime
from werkzeug.utils import secure_filename
import logging
import traceback
import uuid
import json
import time
import re
import random

# è®¾ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Flaskåº”ç”¨é…ç½®
app = Flask(__name__, 
           template_folder=str(project_root / "templates"),
           static_folder=str(project_root / "static"),
           static_url_path='/static')
app.secret_key = os.environ.get('SECRET_KEY', 'your-secret-key-here-change-this-in-production')

# å…¨å±€å˜é‡å­˜å‚¨åˆ†æå™¨å®ä¾‹
analyzer = None
qa_system = None
keyword_search_engine = None
keyword_processor = None
system_status = {}

def process_markdown_answer(answer_text):
    """å¤„ç†markdownç­”æ¡ˆçš„æµå¼è¾“å‡º"""
    # é¢„å¤„ç†ï¼šç¡®ä¿åˆ—è¡¨æ ¼å¼æ­£ç¡®
    processed_answer = answer_text
    
    # å¤„ç†æ•°å­—åˆ—è¡¨ï¼šç¡®ä¿æ¯ä¸ªæ•°å­—åˆ—è¡¨é¡¹ç‹¬å ä¸€è¡Œï¼Œä½†ä¿æŒåŸå§‹æ•°å­—
    processed_answer = re.sub(r'(\d+\.\s+)', r'\n\1', processed_answer)
    
    # å¤„ç†æ— åºåˆ—è¡¨ï¼šç¡®ä¿æ¯ä¸ªåˆ—è¡¨é¡¹ç‹¬å ä¸€è¡Œï¼ŒåŒ…æ‹¬*ç¬¦å·
    processed_answer = re.sub(r'([-â€¢*]\s+)', r'\n\1', processed_answer)
    
    # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
    processed_answer = re.sub(r'\n{3,}', '\n\n', processed_answer)
    
    # æŒ‰è¡Œåˆ†å‰²ï¼Œä¿æŒåŸå§‹ç»“æ„
    lines = processed_answer.split('\n')
    
    for line_idx, line in enumerate(lines):
        line = line.strip()
        if not line:
            yield f"data: {json.dumps({'type': 'answer_chunk', 'content': '\n'}, ensure_ascii=False)}\n\n"
            time.sleep(0.1)
            continue
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ—è¡¨é¡¹
        if re.match(r'^(-|â€¢|\*)', line):
            yield f"data: {json.dumps({'type': 'answer_chunk', 'content': line + '\n'}, ensure_ascii=False)}\n\n"
            time.sleep(0.2)
        elif re.match(r'^\d+\.\s+', line):
            yield f"data: {json.dumps({'type': 'answer_chunk', 'content': line + '\n'}, ensure_ascii=False)}\n\n"
            time.sleep(0.2)
        else:
            sentences = re.split(r'([ã€‚ï¼ï¼Ÿï¼›])', line)
            current_chunk = ""
            for i, part in enumerate(sentences):
                current_chunk += part
                if part in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›'] or i == len(sentences) - 1:
                    if current_chunk.strip():
                        yield f"data: {json.dumps({'type': 'answer_chunk', 'content': current_chunk}, ensure_ascii=False)}\n\n"
                        current_chunk = ""
                        time.sleep(0.25)

def run_vector_rebuild(force: bool = False, quiet: bool = False):
    """æ‰§è¡Œå‘é‡é‡å»º"""
    if not quiet:
        print("ğŸ”§ æ‰§è¡Œå‘é‡é‡å»º...")
    
    notes_directory = os.path.join(project_root, "notes")
    if not os.path.exists(notes_directory):
        notes_directory = str(project_root)
    
    result = startup_vector_rebuild(
        notes_directory=notes_directory,
        force_full_rebuild=force,
        auto_mode=True
    )
    
    if result["success"]:
        if not quiet:
            print(f"âœ… å‘é‡é‡å»ºæˆåŠŸ - {result.get('processed_files_count', 0)} ä¸ªæ–‡ä»¶ï¼Œ"
                  f"{result.get('total_vectors', 0)} ä¸ªå‘é‡")
        return True
    else:
        if not quiet:
            print(f"âŒ å‘é‡é‡å»ºå¤±è´¥: {result.get('error', 'Unknown error')}")
        return False

def auto_rebuild_vectors():
    """å¯åŠ¨æ—¶è‡ªåŠ¨é‡å»ºå‘é‡ç´¢å¼•"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æ—¶è‡ªåŠ¨å‘é‡é‡å»º...")
        
        notes_dir = os.path.join(project_root, "notes")
        
        rebuild_result = startup_vector_rebuild(
            notes_directory=notes_dir,
            force_full_rebuild=False,
            auto_mode=True
        )
        
        if rebuild_result["success"]:
            logger.info("âœ… å‘é‡é‡å»ºæˆåŠŸ")
            system_status['vector_rebuild'] = {
                'status': 'success',
                'processed_files': rebuild_result.get('processed_files_count', 0),
                'total_vectors': rebuild_result.get('total_vectors', 0),
                'duration': rebuild_result.get('duration_seconds', 0),
                'type': rebuild_result.get('rebuild_type', 'incremental'),
                'timestamp': rebuild_result.get('timestamp', '')
            }
            
            if 'scan_result' in rebuild_result:
                scan = rebuild_result['scan_result']
                logger.info(f"ğŸ“Š æ–‡ä»¶å˜åŒ–ç»Ÿè®¡ - æ–°å¢: {len(scan['added'])}, "
                           f"ä¿®æ”¹: {len(scan['modified'])}, åˆ é™¤: {len(scan['deleted'])}")
                system_status['vector_rebuild']['scan_summary'] = {
                    'added': len(scan['added']),
                    'modified': len(scan['modified']),
                    'deleted': len(scan['deleted']),
                    'unchanged': len(scan['unchanged'])
                }
        else:
            logger.error(f"âŒ å‘é‡é‡å»ºå¤±è´¥: {rebuild_result.get('error', 'Unknown error')}")
            system_status['vector_rebuild'] = {
                'status': 'failed',
                'error': rebuild_result.get('error', 'Unknown error'),
                'timestamp': datetime.now().isoformat()
            }
        
        return rebuild_result
        
    except Exception as e:
        logger.error(f"è‡ªåŠ¨å‘é‡é‡å»ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        system_status['vector_rebuild'] = {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
        return {"success": False, "error": str(e)}

def init_keyword_system():
    """åˆå§‹åŒ–å…³é”®è¯æ£€ç´¢ç³»ç»Ÿ"""
    global keyword_search_engine, keyword_processor, system_status
    
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–å…³é”®è¯æ£€ç´¢ç³»ç»Ÿ...")
        
        keyword_search_engine = KeywordSearchEngine()
        keyword_processor = KeywordProcessor()
        notes_directory = os.path.join(project_root, "notes")
        
        if os.path.exists(notes_directory):
            result = keyword_processor.scan_and_process_directory(notes_directory)
            
            if result['success']:
                stats = result['statistics']
                system_status['keyword_system'] = {
                    'status': 'ready',
                    'processed_files': len(result['processed_files']),
                    'failed_files': len(result['failed_files']),
                    'skipped_files': result['skipped_files'],
                    'total_sentences': stats['sentences'],
                    'total_keywords': stats['keywords'],
                    'total_files': stats['files'],
                    'timestamp': datetime.now().isoformat()
                }
                logger.info(f"âœ… å…³é”®è¯ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ - å¤„ç†äº† {len(result['processed_files'])} ä¸ªæ–‡ä»¶")
                logger.info(f"ğŸ“Š å…³é”®è¯ç»Ÿè®¡ - å¥å­: {stats['sentences']}, å…³é”®è¯: {stats['keywords']}")
                return True
            else:
                system_status['keyword_system'] = {
                    'status': 'error',
                    'error': result.get('error', 'Unknown error'),
                    'timestamp': datetime.now().isoformat()
                }
                logger.error(f"âŒ å…³é”®è¯ç³»ç»Ÿå¤„ç†å¤±è´¥: {result.get('error', 'Unknown error')}")
                return False
        else:
            system_status['keyword_system'] = {
                'status': 'warning',
                'message': f'ç¬”è®°ç›®å½•ä¸å­˜åœ¨: {notes_directory}',
                'timestamp': datetime.now().isoformat()
            }
            logger.warning(f"âš ï¸ ç¬”è®°ç›®å½•ä¸å­˜åœ¨: {notes_directory}")
            return False
        
    except Exception as e:
        logger.error(f"å…³é”®è¯ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        system_status['keyword_system'] = {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
        return False

def init_system():
    """åˆå§‹åŒ–ç³»ç»Ÿ"""
    global analyzer, qa_system, keyword_search_engine, keyword_processor, system_status
    
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½é—®ç­”ç³»ç»Ÿ...")
        
        # 1. è‡ªåŠ¨å‘é‡é‡å»º
        print("ğŸ”§ æ­¥éª¤ 1/4: è‡ªåŠ¨å‘é‡é‡å»º")
        print("   ğŸ“‚ æ‰«ænotesç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶...")
        logger.info("=" * 60)
        logger.info("ğŸ”§ ç¬¬ä¸€æ­¥ï¼šè‡ªåŠ¨å‘é‡é‡å»º")
        logger.info("=" * 60)
        vector_result = auto_rebuild_vectors()
        
        if vector_result.get('success'):
            processed_count = vector_result.get('processed_files_count', 0)
            total_vectors = vector_result.get('total_vectors', 0)
            print(f"   âœ… å‘é‡é‡å»ºå®Œæˆ - å¤„ç†äº† {processed_count} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {total_vectors} ä¸ªå‘é‡")
        else:
            print(f"   âŒ å‘é‡é‡å»ºå¤±è´¥: {vector_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return False
        
        # 2. åˆå§‹åŒ–å…³é”®è¯æ£€ç´¢ç³»ç»Ÿ
        print("ğŸ”§ æ­¥éª¤ 2/4: æ„å»ºå…³é”®è¯ç´¢å¼•")
        print("   ğŸ” åˆ†ææ–‡æœ¬å†…å®¹ï¼Œæå–å…³é”®è¯...")
        logger.info("=" * 60)
        logger.info("ğŸ”§ ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–å…³é”®è¯æ£€ç´¢ç³»ç»Ÿ")
        logger.info("=" * 60)
        keyword_result = init_keyword_system()
        
        if keyword_result:
            keyword_stats = system_status.get('keyword_system', {})
            sentences_count = keyword_stats.get('total_sentences', 0)
            keywords_count = keyword_stats.get('total_keywords', 0)
            files_count = keyword_stats.get('processed_files', 0)
            print(f"   âœ… å…³é”®è¯ç´¢å¼•å®Œæˆ - å¤„ç†äº† {files_count} ä¸ªæ–‡ä»¶ï¼Œ{sentences_count} ä¸ªå¥å­ï¼Œ{keywords_count} ä¸ªå…³é”®è¯")
        else:
            print("   âŒ å…³é”®è¯ç´¢å¼•æ„å»ºå¤±è´¥")
            return False
        
        # 3. åˆå§‹åŒ–æ–‡æœ¬åˆ†æå™¨
        print("ğŸ”§ æ­¥éª¤ 3/4: åˆå§‹åŒ–æ–‡æœ¬åˆ†æå™¨å’Œé—®ç­”ç³»ç»Ÿ")
        print("   ğŸ§  è¿æ¥å‘é‡æ•°æ®åº“å’ŒAIæ¨¡å‹...")
        logger.info("=" * 60)
        logger.info("ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šåˆå§‹åŒ–æ–‡æœ¬åˆ†æå™¨")
        logger.info("=" * 60)
        analyzer = ChineseTextAnalyzer(
            model_name="quentinz/bge-large-zh-v1.5",
            use_ollama=True,
            qdrant_host="localhost",
            qdrant_port=6333,
            collection_name="smart_notes"
        )
        
        # 4. æ£€æŸ¥æ–‡æ¡£åº“çŠ¶æ€
        stats = analyzer.get_collection_stats()
        system_status['vector_stats'] = stats
        
        if stats.get('total_points', 0) == 0:
            print("   âŒ å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­åˆå§‹åŒ–")
            logger.warning("å‘é‡åº“ä¸ºç©ºï¼Œéœ€è¦å¤„ç†æ–‡æ¡£")
            system_status['warning'] = "å‘é‡åº“ä¸ºç©ºï¼Œè¯·å…ˆå¤„ç†æ–‡æ¡£"
            return False
        else:
            total_vectors = stats.get('total_points', 0)
            vector_dim = stats.get('vector_size', 0)
            print(f"   âœ… å‘é‡åº“çŠ¶æ€æ­£å¸¸ - {total_vectors} ä¸ªå‘é‡ï¼Œ{vector_dim} ç»´")
            logger.info(f"ğŸ“Š å‘é‡åº“çŠ¶æ€ - æ€»å‘é‡æ•°: {total_vectors}, ç»´åº¦: {vector_dim}")
        
        # 5. åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ
        print("ğŸ”§ æ­¥éª¤ 4/4: åˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ")
        print("   ğŸ¤– è¿æ¥AIæ¨¡å‹ (qwen3:14b)...")
        logger.info("=" * 60)
        logger.info("ğŸ”§ ç¬¬å››æ­¥ï¼šåˆå§‹åŒ–é—®ç­”ç³»ç»Ÿ")
        logger.info("=" * 60)
        qa_success = analyzer.init_qa_system("qwen3:14b")
        if qa_success:
            qa_system = analyzer.qa_system
            system_status['qa_ready'] = True
            print("   âœ… é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            logger.info("âœ… é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
        else:
            system_status['qa_ready'] = False
            system_status['error'] = "é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥"
            print("   âŒ é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            logger.error("âŒ é—®ç­”ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥")
            return False
            
        system_status['analyzer_ready'] = True
        system_status['last_init'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        logger.info("=" * 60)
        
        return True
        
    except Exception as e:
        logger.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        system_status['error'] = str(e)
        system_status['analyzer_ready'] = False
        return False

def show_status():
    """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
    print("ğŸ“Š ç³»ç»ŸçŠ¶æ€æ£€æŸ¥")
    print("=" * 50)
    
    try:
        notes_directory = os.path.join(project_root, "notes")
        if not os.path.exists(notes_directory):
            notes_directory = str(project_root)
        manager = VectorIndexManager(notes_directory)
        status = manager.get_system_status()
        
        print(f"ğŸ“ Notesç›®å½•: {status['notes_directory']}")
        print(f"ğŸ“ ç›®å½•å­˜åœ¨: {'âœ…' if status['notes_directory_exists'] else 'âŒ'}")
        print(f"ğŸ¤– åˆ†æå™¨åˆå§‹åŒ–: {'âœ…' if status['analyzer_initialized'] else 'âŒ'}")
        print(f"ğŸ“ è·Ÿè¸ªæ–‡ä»¶æ•°: {status['total_files_tracked']}")
        print(f"ğŸ• ä¸Šæ¬¡æ‰«æ: {status['last_scan']}")
        print(f"ğŸ“„ çŠ¶æ€æ–‡ä»¶: {'âœ…' if status['state_file_exists'] else 'âŒ'}")
        
        if 'vector_stats' in status:
            stats = status['vector_stats']
            print(f"ğŸ“Š å‘é‡ç»Ÿè®¡:")
            print(f"   æ€»å‘é‡æ•°: {stats.get('total_points', 0)}")
            print(f"   å‘é‡ç»´åº¦: {stats.get('vector_size', 0)}")
            print(f"   è·ç¦»åº¦é‡: {stats.get('distance_metric', 'N/A')}")
        
        if 'cache_info' in status:
            cache = status['cache_info']
            print(f"ğŸ’¾ ç¼“å­˜ä¿¡æ¯:")
            print(f"   ç¼“å­˜æ–‡ä»¶æ•°: {cache.get('total_cached_files', 0)}")
            print(f"   ç¼“å­˜æ–‡ä»¶å­˜åœ¨: {'âœ…' if cache.get('cache_file_exists', False) else 'âŒ'}")
        
        print("\nğŸ” æ‰§è¡Œæ–‡ä»¶æ‰«æ...")
        scan_result = manager.scan_notes_directory()
        
        print(f"ğŸ“‹ æ–‡ä»¶å˜åŒ–ç»Ÿè®¡:")
        print(f"   æ–°å¢: {len(scan_result['added'])} ä¸ª")
        print(f"   ä¿®æ”¹: {len(scan_result['modified'])} ä¸ª") 
        print(f"   åˆ é™¤: {len(scan_result['deleted'])} ä¸ª")
        print(f"   æœªå˜åŒ–: {len(scan_result['unchanged'])} ä¸ª")
        print(f"   æ€»æ–‡ä»¶æ•°: {scan_result['total_current']}")
        
    except Exception as e:
        print(f"âŒ çŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")

def start_demo(auto_rebuild: bool = True):
    """å¯åŠ¨å‘½ä»¤è¡Œé—®ç­”æ¼”ç¤º"""
    print("ğŸ’¬ å¯åŠ¨å‘½ä»¤è¡Œé—®ç­”æ¼”ç¤º...")
    print("=" * 50)
    
    if auto_rebuild:
        print("ç¬¬1æ­¥: å‘é‡é‡å»º")
        rebuild_success = run_vector_rebuild(quiet=True)
        if not rebuild_success:
            print("âš ï¸ å‘é‡é‡å»ºå¤±è´¥ï¼Œé—®ç­”åŠŸèƒ½å¯èƒ½å—å½±å“")
        print()
    
    print("ç¬¬2æ­¥: å¯åŠ¨é—®ç­”æ¼”ç¤º")
    print("=" * 50)
    
    demos = {
        '1': ('ollama_demo.py', 'Ollamaæœ¬åœ°æ¨¡å‹ç‰ˆæœ¬'),
        '2': ('text_analyzer.py', 'å®Œæ•´ç‰ˆQdrantç³»ç»Ÿ'),
        '3': ('simple_demo.py', 'Sentence Transformersç‰ˆæœ¬'),
        '4': ('local_demo.py', 'æœ¬åœ°TF-IDFç‰ˆæœ¬')
    }
    
    print("è¯·é€‰æ‹©æ¼”ç¤ºç‰ˆæœ¬:")
    for key, (filename, desc) in demos.items():
        print(f"{key}. {desc}")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (1-4, é»˜è®¤1): ").strip() or '1'
    
    if choice in demos:
        demo_file, desc = demos[choice]
        print(f"\nğŸš€ å¯åŠ¨ {desc}...")
        try:
            subprocess.run([sys.executable, str(project_root / demo_file)], check=True)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ æ¼”ç¤ºå·²åœæ­¢")
        except Exception as e:
            print(f"âŒ æ¼”ç¤ºå¯åŠ¨å¤±è´¥: {e}")
    else:
        print("âŒ æ— æ•ˆé€‰é¡¹")

def start_cache_manager():
    """å¯åŠ¨ç¼“å­˜ç®¡ç†å·¥å…·"""
    print("ğŸ“¦ å¯åŠ¨ç¼“å­˜ç®¡ç†å·¥å…·...")
    print("=" * 50)
    
    try:
        subprocess.run([sys.executable, str(project_root / "cache_manager.py")], check=True)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¼“å­˜ç®¡ç†å·¥å…·å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ ç¼“å­˜ç®¡ç†å·¥å…·å¯åŠ¨å¤±è´¥: {e}")

# Flaskè·¯ç”±å®šä¹‰
@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html', system_status=system_status)

@app.route('/api/status')
def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    try:
        status = {
            'system_ready': system_status.get('analyzer_ready', False),
            'qa_ready': system_status.get('qa_ready', False),
            'last_init': system_status.get('last_init', 'Never'),
            'error': system_status.get('error'),
            'warning': system_status.get('warning')
        }
        
        if analyzer:
            stats = analyzer.get_collection_stats()
            status.update({
                'document_count': stats.get('total_points', 0),
                'vector_size': stats.get('vector_size', 0),
                'collection_name': stats.get('collection_name', 'Unknown')
            })
            
            cache_info = analyzer.get_cache_info()
            status.update({
                'cached_files': cache_info['total_cached_files'],
                'cache_path': cache_info['cache_file_path']
            })
            
            status.update({
                'embedding_model': analyzer.model_name,
                'llm_model': qa_system.llm.model_name if qa_system else 'Not initialized'
            })
        
        return jsonify(status)
        
    except Exception as e:
        logger.error(f"è·å–çŠ¶æ€é”™è¯¯: {e}")
        return jsonify({
            'system_ready': False,
            'error': str(e)
        })

@app.route('/api/ask_stream', methods=['POST'])
def ask_question_stream():
    """æµå¼å¤„ç†é—®ç­”è¯·æ±‚"""
    if not qa_system:
        return jsonify({
            'success': False,
            'error': 'é—®ç­”ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ'
        })
    
    try:
        data = request.get_json()
        question = data.get('question', '').strip()
        show_thinking = data.get('show_thinking', False)
        
        if not question:
            return jsonify({
                'success': False,
                'error': 'è¯·è¾“å…¥é—®é¢˜'
            })
        
        if 'session_id' not in session:
            session['session_id'] = str(uuid.uuid4())
        
        def generate_stream():
            try:
                yield f"data: {json.dumps({'type': 'start', 'question': question}, ensure_ascii=False)}\n\n"
                time.sleep(0.1)
                
                # 1. å…³é”®è¯æå–
                yield f"data: {json.dumps({'type': 'step', 'step': 'æå–å…³é”®è¯', 'message': 'ğŸ” æ­£åœ¨æå–å…³é”®è¯...'}, ensure_ascii=False)}\n\n"
                time.sleep(0.5)
                
                keywords = qa_system.extract_search_keywords(question)
                yield f"data: {json.dumps({'type': 'step_complete', 'step': 'æå–å…³é”®è¯', 'message': f'âœ… å…³é”®è¯æå–å®Œæˆ: {', '.join(keywords)}'}, ensure_ascii=False)}\n\n"
                time.sleep(0.3)
                
                # 2. å‘é‡æ£€ç´¢
                yield f"data: {json.dumps({'type': 'step', 'step': 'æ£€ç´¢æ–‡æ¡£', 'message': 'ğŸ“š æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£...'}, ensure_ascii=False)}\n\n"
                time.sleep(0.5)
                
                context_results = qa_system.get_relevant_context(keywords, max_results=3)
                sources = []
                for result in context_results:
                    sources.append({
                        'filename': result['metadata'].get('filename', 'æœªçŸ¥'),
                        'category': result['metadata'].get('category', 'æœªåˆ†ç±»'),
                        'score': result['score']
                    })
                
                yield f"data: {json.dumps({'type': 'step_complete', 'step': 'æ£€ç´¢æ–‡æ¡£', 'message': f'âœ… æ–‡æ¡£æ£€ç´¢å®Œæˆ: æ‰¾åˆ° {len(sources)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ', 'sources': sources}, ensure_ascii=False)}\n\n"
                time.sleep(0.3)
                
                # 3. ç”Ÿæˆç­”æ¡ˆ
                if show_thinking:
                    yield f"data: {json.dumps({'type': 'step', 'step': 'AIæ€è€ƒ', 'message': 'ğŸ¤” AIæ­£åœ¨æ·±åº¦æ€è€ƒ...'}, ensure_ascii=False)}\n\n"
                    time.sleep(0.3)
                    yield f"data: {json.dumps({'type': 'thinking_start', 'step': 'AIæ€è€ƒ'}, ensure_ascii=False)}\n\n"
                
                result = qa_system.answer_question(question, max_results=10)
                response = result['answer']
                
                yield f"data: {json.dumps({'type': 'answer_start'}, ensure_ascii=False)}\n\n"
                yield from process_markdown_answer(response)
                
                # æ·»åŠ å¼•ç”¨æ¥æº
                if sources:
                    yield f"data: {json.dumps({'type': 'answer_chunk', 'content': '\n\n---\n\n**ğŸ“š å‚è€ƒæ¥æºï¼š**\n\n'}, ensure_ascii=False)}\n\n"
                    time.sleep(0.1)
                    for i, source in enumerate(sources, 1):
                        source_text = f"**{source['filename']}** (ç›¸ä¼¼åº¦: {source['score']:.1%})\n"
                        yield f"data: {json.dumps({'type': 'answer_chunk', 'content': source_text}, ensure_ascii=False)}\n\n"
                        time.sleep(0.1)
                
                # è®¡ç®—ç½®ä¿¡åº¦
                avg_score = sum([r['score'] for r in context_results]) / len(context_results) if context_results else 0
                confidence = min(avg_score * 2, 1.0)
                
                yield f"data: {json.dumps({'type': 'complete', 'confidence': confidence, 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
                
            except Exception as e:
                logger.error(f"æµå¼é—®ç­”é”™è¯¯: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"
                yield "data: [DONE]\n\n"
        
        return Response(generate_stream(), mimetype='text/plain', headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'X-Accel-Buffering': 'no'
        })
        
    except Exception as e:
        logger.error(f"æµå¼é—®ç­”åˆå§‹åŒ–é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        })

# ç®€åŒ–ç‰ˆAPI - åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½
@app.route('/api/ask', methods=['POST'])
def ask_question():
    """å¤„ç†é—®ç­”è¯·æ±‚"""
    if not qa_system:
        return jsonify({
            'success': False,
            'error': 'é—®ç­”ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåˆå§‹åŒ–ç³»ç»Ÿ'
        })
    
    try:
        data = request.get_json()
        question = data.get('question', '').strip()
        
        if not question:
            return jsonify({
                'success': False,
                'error': 'è¯·è¾“å…¥é—®é¢˜'
            })
        
        if 'session_id' not in session:
            session['session_id'] = str(uuid.uuid4())
        
        result = qa_system.answer_question(question, max_results=10)
        
        response = {
            'success': True,
            'question': question,
            'answer': result['answer'],
            'confidence': result['confidence'],
            'sources': result['sources'],
            'context_count': result.get('context_count', 0),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'session_id': session['session_id']
        }
        
        logger.info(f"é—®ç­”å®Œæˆ: {question[:50]}...")
        return jsonify(response)
        
    except Exception as e:
        logger.error(f"é—®ç­”å¤„ç†é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}'
        })

@app.route('/api/document_list')
def get_document_list():
    """è·å–å·²å¯¼å…¥çš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        if not analyzer:
            return jsonify({
                'success': False,
                'error': 'åˆ†æå™¨æœªåˆå§‹åŒ–'
            })
        
        documents = analyzer.get_document_list()
        
        return jsonify({
            'success': True,
            'documents': documents,
            'total_count': len(documents),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}'
        })

@app.route('/api/keyword_stats')
def get_keyword_stats():
    """è·å–å…³é”®è¯ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if not keyword_search_engine:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        # ä½¿ç”¨å…³é”®è¯æœç´¢å¼•æ“è·å–ç»Ÿè®¡ä¿¡æ¯
        statistics = keyword_search_engine.get_statistics()
        
        return jsonify({
            'success': True,
            'statistics': statistics,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–å…³é”®è¯ç»Ÿè®¡é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–å…³é”®è¯ç»Ÿè®¡å¤±è´¥: {str(e)}'
        })

@app.route('/api/top_keywords')
def get_top_keywords():
    """è·å–çƒ­é—¨å…³é”®è¯"""
    try:
        if not keyword_search_engine:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        # è·å–limitå‚æ•°ï¼Œé»˜è®¤ä¸º50
        limit = request.args.get('limit', 50, type=int)
        
        # é™åˆ¶æœ€å¤§æ•°é‡ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¤§
        if limit > 200:
            limit = 200
        
        # ä½¿ç”¨å…³é”®è¯æœç´¢å¼•æ“è·å–çƒ­é—¨å…³é”®è¯
        top_keywords = keyword_search_engine.get_top_keywords(limit)
        
        return jsonify({
            'success': True,
            'keywords': top_keywords,
            'total_count': len(top_keywords),
            'limit': limit,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–çƒ­é—¨å…³é”®è¯é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–çƒ­é—¨å…³é”®è¯å¤±è´¥: {str(e)}'
        })

@app.route('/api/keyword_search', methods=['POST'])
def search_keywords():
    """å…³é”®è¯æœç´¢"""
    try:
        if not keyword_search_engine:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        data = request.get_json()
        query = data.get('query', '').strip()
        limit = data.get('limit', 20)
        
        if not query:
            return jsonify({
                'success': False,
                'error': 'è¯·è¾“å…¥æœç´¢å…³é”®è¯'
            })
        
        # é™åˆ¶æœ€å¤§æ•°é‡
        if limit > 100:
            limit = 100
        
        # æ‰§è¡Œå…³é”®è¯æœç´¢
        search_results = keyword_search_engine.search_keywords(query, limit)
        
        # è½¬æ¢æ•°æ®æ ¼å¼ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
        keywords = []
        for result in search_results:
            keywords.append({
                'keyword': result['word'],  # å‰ç«¯æœŸæœ›keywordå­—æ®µ
                'count': result['frequency'],  # å‰ç«¯æœŸæœ›countå­—æ®µ
                'sentence_count': result['sentence_count']
            })
        
        return jsonify({
            'success': True,
            'keywords': keywords,  # å‰ç«¯æœŸæœ›keywordså­—æ®µè€Œä¸æ˜¯results
            'query': query,
            'total_count': len(keywords),
            'limit': limit,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"å…³é”®è¯æœç´¢é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'å…³é”®è¯æœç´¢å¤±è´¥: {str(e)}'
        })

@app.route('/api/get_sentences', methods=['POST'])
def get_sentences_by_keywords():
    """æ ¹æ®å…³é”®è¯è·å–ç›¸å…³å¥å­"""
    try:
        if not keyword_search_engine:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        data = request.get_json()
        keywords = data.get('keywords', [])
        limit = data.get('limit', 100)
        
        if not keywords:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›å…³é”®è¯åˆ—è¡¨'
            })
        
        # ç¡®ä¿keywordsæ˜¯åˆ—è¡¨
        if isinstance(keywords, str):
            keywords = [keywords]
        
        # é™åˆ¶æœ€å¤§æ•°é‡
        if limit > 500:
            limit = 500
        
        # è·å–åŒ…å«å…³é”®è¯çš„å¥å­
        raw_sentences = keyword_search_engine.get_sentences_by_keywords(keywords)
        
        # å¦‚æœæœ‰é™åˆ¶ï¼Œåˆ™æˆªå–
        if limit and len(raw_sentences) > limit:
            raw_sentences = raw_sentences[:limit]
        
        # è½¬æ¢æ•°æ®æ ¼å¼ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
        sentences = []
        for sentence in raw_sentences:
            # å®‰å…¨å¤„ç†æ‰€æœ‰å­—æ®µï¼Œé¿å…Noneå€¼å¯¼è‡´çš„é”™è¯¯
            if not sentence or not isinstance(sentence, dict):
                continue  # è·³è¿‡æ— æ•ˆçš„å¥å­å¯¹è±¡
            
            text = sentence.get('sentence', '') or ''
            source_file = sentence.get('source_file', '') or ''
            
            # ç¡®ä¿åŸºæœ¬å­—æ®µä¸ä¸ºç©º
            if not text:
                continue  # è·³è¿‡æ²¡æœ‰å†…å®¹çš„å¥å­
            
            sentences.append({
                'content': text,  # å‰ç«¯æœŸæœ›contentå­—æ®µ
                'filename': source_file,  # å‰ç«¯æœŸæœ›filenameå­—æ®µ
                'source_file': source_file,
                'matched_keywords': sentence.get('matched_keywords', []),
                'match_count': sentence.get('match_count', 0),
                'sentence_id': sentence.get('sentence_id', '')
            })
        
        return jsonify({
            'success': True,
            'sentences': sentences,
            'keywords': keywords,
            'total_count': len(sentences),
            'limit': limit,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–å¥å­é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–å¥å­å¤±è´¥: {str(e)}'
        })

@app.route('/api/rebuild_keyword_index', methods=['POST'])
def rebuild_keyword_index():
    """é‡å»ºå…³é”®è¯ç´¢å¼•"""
    try:
        if not keyword_processor:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        # æ‰§è¡Œå…³é”®è¯ç´¢å¼•é‡å»º
        notes_directory = os.path.join(project_root, "notes")
        
        if not os.path.exists(notes_directory):
            return jsonify({
                'success': False,
                'error': f'ç¬”è®°ç›®å½•ä¸å­˜åœ¨: {notes_directory}'
            })
        
        # æ‰«æå¹¶å¤„ç†ç›®å½•
        result = keyword_processor.scan_and_process_directory(notes_directory)
        
        if result['success']:
            stats = result['statistics']
            
            # æ›´æ–°ç³»ç»ŸçŠ¶æ€
            system_status['keyword_system'] = {
                'status': 'ready',
                'processed_files': len(result['processed_files']),
                'failed_files': len(result['failed_files']),
                'skipped_files': result['skipped_files'],
                'total_sentences': stats['sentences'],
                'total_keywords': stats['keywords'],
                'total_files': stats['files'],
                'timestamp': datetime.now().isoformat()
            }
            
            return jsonify({
                'success': True,
                'processed_files': len(result['processed_files']),
                'total_sentences': stats['sentences'],
                'total_keywords': stats['keywords'],
                'failed_files': len(result['failed_files']),
                'skipped_files': result['skipped_files'],
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        else:
            return jsonify({
                'success': False,
                'error': result.get('error', 'å…³é”®è¯ç´¢å¼•é‡å»ºå¤±è´¥')
            })
        
    except Exception as e:
        logger.error(f"é‡å»ºå…³é”®è¯ç´¢å¼•é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'é‡å»ºå…³é”®è¯ç´¢å¼•å¤±è´¥: {str(e)}'
        })

@app.route('/api/random_quote', methods=['POST'])
def get_random_quote():
    """è·å–éšæœºå¥å­"""
    try:
        if not keyword_search_engine:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        data = request.get_json() or {}
        selected_books = data.get('selected_books', [])
        limit = data.get('limit', 1)
        
        # é™åˆ¶æœ€å¤§æ•°é‡
        if limit > 10:
            limit = 10
        
        # è·å–éšæœºå¥å­
        quotes = keyword_search_engine.get_random_quote(selected_books, limit)
        
        if not quotes:
            return jsonify({
                'success': False,
                'error': 'æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„å¥å­'
            })
        
        # å¦‚æœåªè¦æ±‚ä¸€ä¸ªå¥å­ï¼Œç›´æ¥è¿”å›ç¬¬ä¸€ä¸ªï¼Œè½¬æ¢æ ¼å¼ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
        if quotes:
            quote = quotes[0]
            # å®‰å…¨å¤„ç†source_fileï¼Œé¿å…Noneå€¼å¯¼è‡´çš„é”™è¯¯
            source_file = quote.get('source_file', '') or ''
            book_name = source_file.replace('.txt', '').replace('.md', '') if source_file else 'æœªçŸ¥æ¥æº'
            
            formatted_quote = {
                'content': quote.get('text', ''),  # å‰ç«¯æœŸæœ›contentå­—æ®µ
                'book_name': book_name,  # å‰ç«¯æœŸæœ›book_nameå­—æ®µ
                'source_file': source_file,
                'sentence_id': quote.get('sentence_id', '')
            }
        else:
            formatted_quote = None
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = keyword_search_engine.get_statistics()
        
        return jsonify({
            'success': True,
            'quote': formatted_quote,
            'stats': stats,
            'selected_books': selected_books,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–éšæœºå¥å­é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–éšæœºå¥å­å¤±è´¥: {str(e)}'
        })

@app.route('/api/book_list')
def get_book_list():
    """è·å–ä¹¦ç±åˆ—è¡¨"""
    try:
        if not keyword_search_engine:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        # è·å–ä¹¦ç±åˆ—è¡¨
        books_data = keyword_search_engine.get_book_list()
        
        # è½¬æ¢æ•°æ®æ ¼å¼ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
        formatted_books = []
        for book in books_data:
            # å°è¯•è·å–æ–‡ä»¶ä¿¡æ¯æ¥è®¡ç®—å¤§å°å’Œå­—ç¬¦æ•°
            file_path = os.path.join(project_root, "notes", book['source_file'])
            file_size = 0
            char_count = 0
            
            if os.path.exists(file_path):
                try:
                    file_size = os.path.getsize(file_path)
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        char_count = len(content)
                except Exception as e:
                    logger.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
            
            formatted_books.append({
                'book_name': book['name'],  # å‰ç«¯æœŸæœ›book_nameå­—æ®µ
                'name': book['name'],       # å¤‡ç”¨å­—æ®µ
                'filename': book['source_file'],  # å‰ç«¯æœŸæœ›filenameå­—æ®µ
                'source_file': book['source_file'],
                'sentence_count': book['sentence_count'],
                'file_size': file_size,     # å®é™…æ–‡ä»¶å¤§å°
                'size': file_size,          # å¤‡ç”¨å­—æ®µå
                'char_count': char_count,   # å­—ç¬¦æ•°
                'modified_time': None       # å¯ä»¥åç»­æ·»åŠ ä¿®æ”¹æ—¶é—´ä¿¡æ¯
            })
        
        return jsonify({
            'success': True,
            'books': formatted_books,
            'total_count': len(formatted_books),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–ä¹¦ç±åˆ—è¡¨é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–ä¹¦ç±åˆ—è¡¨å¤±è´¥: {str(e)}'
        })

@app.route('/api/book_content', methods=['POST'])
def get_book_content():
    """è·å–ä¹¦ç±å†…å®¹"""
    try:
        if not keyword_search_engine:
            return jsonify({
                'success': False,
                'error': 'å…³é”®è¯ç³»ç»Ÿæœªåˆå§‹åŒ–'
            })
        
        data = request.get_json()
        filename = data.get('filename', '').strip()
        
        if not filename:
            return jsonify({
                'success': False,
                'error': 'è¯·æä¾›æ–‡ä»¶å'
            })
        
        # è·å–ä¹¦ç±å†…å®¹
        book_content = keyword_search_engine.get_book_content(filename)
        
        if not book_content or not book_content.get('sentences'):
            return jsonify({
                'success': False,
                'error': f'æœªæ‰¾åˆ°æ–‡ä»¶ {filename} çš„å†…å®¹'
            })
        
        # æ„å»ºè¿”å›æ ¼å¼ï¼Œå…¼å®¹å‰ç«¯æœŸæœ›çš„æ ¼å¼
        content = '\n\n'.join([sentence['text'] for sentence in book_content['sentences']])
        
        # è®¡ç®—æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        line_count = content.count('\n') + 1 if content else 0
        char_count = len(content)
        
        # å°è¯•è·å–å®é™…æ–‡ä»¶å¤§å°
        file_path = os.path.join(project_root, "notes", filename)
        file_size = 0
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
        else:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨å†…å®¹é•¿åº¦ä½œä¸ºè¿‘ä¼¼å€¼
            file_size = len(content.encode('utf-8'))
        
        return jsonify({
            'success': True,
            'book_info': {
                'name': book_content['name'],
                'book_name': book_content['name'],  # å‰ç«¯æœŸæœ›çš„å­—æ®µå
                'source_file': book_content['source_file'],
                'sentence_count': book_content['sentence_count'],
                'line_count': line_count,  # å‰ç«¯æœŸæœ›çš„å­—æ®µ
                'char_count': char_count,  # å‰ç«¯æœŸæœ›çš„å­—æ®µ
                'file_size': file_size     # å‰ç«¯æœŸæœ›çš„å­—æ®µ
            },
            'content': content,
            'sentences': book_content['sentences'],
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        })
        
    except Exception as e:
        logger.error(f"è·å–ä¹¦ç±å†…å®¹é”™è¯¯: {e}")
        return jsonify({
            'success': False,
            'error': f'è·å–ä¹¦ç±å†…å®¹å¤±è´¥: {str(e)}'
        })

def start_web_app(auto_rebuild: bool = True, force_rebuild: bool = False, port: int = 5001, api_only: bool = False):
    """å¯åŠ¨Webåº”ç”¨"""
    mode_desc = "APIæ¨¡å¼" if api_only else "Webç•Œé¢æ¨¡å¼"
    print(f"ğŸŒ å¯åŠ¨æ™ºèƒ½é—®ç­”ç³»ç»Ÿ - {mode_desc}")
    print("=" * 50)
    
    if auto_rebuild:
        print("ç¬¬1æ­¥: å‘é‡é‡å»º")
        rebuild_success = run_vector_rebuild(force=force_rebuild)
        if not rebuild_success:
            print("âš ï¸ å‘é‡é‡å»ºå¤±è´¥ï¼Œä½†ç»§ç»­å¯åŠ¨Webåº”ç”¨...")
        print()
    
    print("ç¬¬2æ­¥: å¯åŠ¨WebæœåŠ¡")
    port_desc = f"http://localhost:{port}"
    print(f"ğŸŒ WebæœåŠ¡åœ°å€: {port_desc}")
    if not api_only:
        print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸Šè¿°åœ°å€")
    else:
        print("ğŸ”Œ APIæœåŠ¡å·²å¯åŠ¨ï¼Œå¯é€šè¿‡HTTPæ¥å£è®¿é—®")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    init_success = init_system()
    
    if init_success:
        print("")
        print("=" * 60)
        print("âœ… ç³»ç»Ÿå®Œæ•´åˆå§‹åŒ–æˆåŠŸï¼")
        print("ğŸ” å‘é‡ç´¢å¼•å·²æ„å»ºå®Œæˆ")
        print("ğŸ”¤ å…³é”®è¯ç´¢å¼•å·²æ„å»ºå®Œæˆ") 
        print("ğŸ§  é—®ç­”ç³»ç»Ÿå·²å°±ç»ª")
        print("=" * 60)
        print("")
        
        # å¯åŠ¨Flaskåº”ç”¨
        try:
            # ä»ç¯å¢ƒå˜é‡è·å–ä¸»æœºé…ç½®ï¼Œé»˜è®¤åªç»‘å®šæœ¬åœ°
            host = os.environ.get('FLASK_HOST', '127.0.0.1')
            app.run(debug=False, host=host, port=port, use_reloader=False)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Webåº”ç”¨å·²åœæ­¢")
    else:
        print("")
        print("=" * 60)
        print("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼")
        print("ğŸ’¡ è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„é—®é¢˜ï¼š")
        print("   - QdrantæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ (docker-compose up -d)")
        print("   - OllamaæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")
        print("   - notesç›®å½•æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«æ–‡ä»¶")
        print("   - ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("=" * 60)
        print("")
        print("ğŸ”§ å°è¯•ä¿®å¤é—®é¢˜åï¼Œè¯·é‡æ–°è¿è¡Œç¨‹åº")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½é—®ç­”ç³»ç»Ÿ - ç»Ÿä¸€åº”ç”¨å…¥å£",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å¯åŠ¨æ¨¡å¼:
  web       å¯åŠ¨Webåº”ç”¨ï¼ˆé»˜è®¤ï¼ŒåŒ…å«è‡ªåŠ¨å‘é‡é‡å»ºï¼‰
  api-only  ä»…å¯åŠ¨Web APIï¼ˆæ— æµè§ˆå™¨ç•Œé¢ï¼‰
  demo      å¯åŠ¨å‘½ä»¤è¡Œé—®ç­”æ¼”ç¤º
  cache     å¯åŠ¨ç¼“å­˜ç®¡ç†å·¥å…·
  rebuild   ä»…æ‰§è¡Œå‘é‡é‡å»º
  status    æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€

ç¤ºä¾‹:
  python smart_notes_app.py                     # å¯åŠ¨Webåº”ç”¨
  python smart_notes_app.py web --no-rebuild    # å¯åŠ¨Webåº”ç”¨ä½†è·³è¿‡å‘é‡é‡å»º
  python smart_notes_app.py api-only --port 8080 # ä»…å¯åŠ¨APIæœåŠ¡åœ¨8080ç«¯å£
  python smart_notes_app.py demo                # å¯åŠ¨é—®ç­”æ¼”ç¤º
  python smart_notes_app.py rebuild --force     # å¼ºåˆ¶å…¨é‡å‘é‡é‡å»º
  python smart_notes_app.py status              # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        """
    )
    
    parser.add_argument("mode", 
                       nargs='?', 
                       default="web",
                       choices=["web", "api-only", "demo", "cache", "rebuild", "status"],
                       help="å¯åŠ¨æ¨¡å¼ (é»˜è®¤: web)")
    
    parser.add_argument("--force", 
                       action="store_true",
                       help="å¼ºåˆ¶å…¨é‡é‡å»ºå‘é‡ï¼ˆé€‚ç”¨äºrebuildæ¨¡å¼ï¼‰")
    
    parser.add_argument("--no-rebuild", 
                       action="store_true",
                       help="è·³è¿‡è‡ªåŠ¨å‘é‡é‡å»ºï¼ˆé€‚ç”¨äºwebå’Œdemoæ¨¡å¼ï¼‰")
    
    parser.add_argument("--port", 
                       type=int,
                       default=5001,
                       help="WebæœåŠ¡ç«¯å£å· (é»˜è®¤: 5001)")
    
    args = parser.parse_args()
    
    print("ğŸš€ æ™ºèƒ½é—®ç­”ç³»ç»Ÿ - ç»Ÿä¸€åº”ç”¨å…¥å£")
    print("=" * 50)
    
    try:
        if args.mode in ["web", "api-only"]:
            start_web_app(
                auto_rebuild=not args.no_rebuild,
                force_rebuild=args.force,
                port=args.port,
                api_only=(args.mode == "api-only")
            )
        elif args.mode == "demo":
            start_demo(auto_rebuild=not args.no_rebuild)
        elif args.mode == "cache":
            start_cache_manager()
        elif args.mode == "rebuild":
            print("ğŸ”§ æ‰§è¡Œå‘é‡é‡å»º...")
            success = run_vector_rebuild(force=args.force)
            if success:
                print("ğŸ‰ å‘é‡é‡å»ºå®Œæˆï¼")
            else:
                print("âŒ å‘é‡é‡å»ºå¤±è´¥ï¼")
                sys.exit(1)
        elif args.mode == "status":
            show_status()
    
    except KeyboardInterrupt:
        print("\nğŸ‘‹ åº”ç”¨å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ åº”ç”¨é”™è¯¯: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
